<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Logistic Regression in Python &mdash; Paolo Mascia</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="article.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
</head>
<body>

    <nav class="navbar scrolled">
        <div class="container">
            <a href="index.html" class="nav-logo">
                <span class="logo-accent">&gt;</span> paolo.mascia
            </a>
            <div class="nav-links">
                <a href="index.html#about">About</a>
                <a href="index.html#projects">Projects</a>
                <a href="ai-tech-insights.html">AI Tech Insights</a>
                <a href="cybersecurity-tech-insights.html">Cybersecurity</a>
                <a href="golang-tech-insights.html">Go</a>
                <a href="index.html#contact">Contact</a>
            </div>
        </div>
    </nav>

    <article class="article">
        <div class="container">
            <div class="article-layout">
                <div class="article-main">
                    <header class="article-header">
                        <a href="ai-tech-insights.html" class="back-link">&larr; Back to Tech Insights</a>
                        <div class="post-meta">
                            <span class="post-date">Jan 2024</span>
                            <span class="post-reading">14 min read</span>
                        </div>
                        <h1>Logistic Regression in Python</h1>
                        <div class="post-tags">
                            <span>Logistic Regression</span>
                            <span>Classification</span>
                            <span>Scikit-Learn</span>
                        </div>
                    </header>

                    <div class="article-body">

                        <p class="lead">Logistic Regression is a linear model for classification that predicts the probability of an observation belonging to a class by fitting a linear function to the log-odds. Despite its name, it is a classification algorithm&mdash;not a regression algorithm&mdash;and remains one of the most widely used and best-understood models in machine learning and statistics.</p>

                        <h2>What Logistic Regression Models</h2>

                        <p>In binary classification we want to estimate <em>P(y = 1 | X)</em>. A linear model applied directly to probabilities would produce values outside [0, 1]. Logistic Regression solves this by modelling the <strong>log-odds</strong> (also called the <strong>logit</strong>) as a linear function of the features:</p>

                        <p style="text-align: center; font-style: italic;">log(p / (1 &minus; p)) = &beta;<sub>0</sub> + &beta;<sub>1</sub>x<sub>1</sub> + &hellip; + &beta;<sub>k</sub>x<sub>k</sub></p>

                        <p>Solving for <em>p</em> gives the <strong>sigmoid function</strong>:</p>

                        <p style="text-align: center; font-style: italic;">p = 1 / (1 + exp(&minus;z)), &nbsp; where z = &beta;<sub>0</sub> + &beta;<sub>1</sub>x<sub>1</sub> + &hellip; + &beta;<sub>k</sub>x<sub>k</sub></p>

                        <p>The sigmoid maps any real-valued <em>z</em> to a probability between 0 and 1. By default, the predicted class is 1 when <em>p</em> &ge; 0.5 and 0 otherwise, but this threshold can be adjusted.</p>

                        <h2>Why Logistic Regression?</h2>

                        <ul>
                            <li><strong>Interpretable:</strong> Each coefficient directly tells you how a one-unit change in a feature affects the log-odds of the positive class. Exponentiated coefficients give odds ratios.</li>
                            <li><strong>Probabilistic:</strong> It outputs calibrated probabilities, not just hard class labels. This is essential for applications like medical diagnosis, credit scoring, and ranking.</li>
                            <li><strong>Efficient:</strong> Training is convex optimisation&mdash;there is a unique global optimum, and modern solvers converge in seconds even on millions of samples.</li>
                            <li><strong>Regularisable:</strong> L1, L2, and Elastic Net penalties are built in, providing feature selection and overfitting control.</li>
                            <li><strong>Well-understood:</strong> Decades of statistical theory support hypothesis testing, confidence intervals, and diagnostic methods.</li>
                        </ul>

                        <h2>Minimal Binary Classification Example</h2>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, classification_report, confusion_matrix,
    ConfusionMatrixDisplay, RocCurveDisplay
)

# Generate a binary classification dataset
X, y = make_classification(
    n_samples=1000, n_features=2, n_redundant=0,
    n_informative=2, random_state=42, n_clusters_per_class=1,
)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Fit logistic regression
model = LogisticRegression(random_state=42)
model.fit(X_train, y_train)

# Evaluate
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]

print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Confusion matrix
fig, axes = plt.subplots(1, 2, figsize=(12, 5))
ConfusionMatrixDisplay.from_predictions(y_test, y_pred, ax=axes[0])
axes[0].set_title("Confusion Matrix")

RocCurveDisplay.from_predictions(y_test, y_prob, ax=axes[1])
axes[1].set_title("ROC Curve")
plt.tight_layout()
plt.show()</code></pre>

                        <p>The model learns a linear decision boundary. With two features, we can visualise it directly:</p>

<pre><code class="language-python"># Visualise decision boundary
h = 0.02
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(
    np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)
)
Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]
Z = Z.reshape(xx.shape)

plt.figure(figsize=(8, 6))
plt.contourf(xx, yy, Z, levels=50, cmap="RdBu_r", alpha=0.8)
plt.colorbar(label="P(y=1)")
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap="RdBu_r",
            edgecolors="k", s=30)
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.title("Logistic Regression Decision Boundary")
plt.tight_layout()
plt.show()</code></pre>

                        <h2>Interpreting Coefficients and Odds Ratios</h2>

                        <p>The coefficients of a logistic regression model have a direct probabilistic interpretation:</p>

<pre><code class="language-python">import pandas as pd

coef_df = pd.DataFrame({
    "Feature": [f"X{i}" for i in range(X.shape[1])],
    "Coefficient": model.coef_[0],
    "Odds Ratio": np.exp(model.coef_[0]),
})
coef_df["Intercept"] = ""
coef_df.loc[len(coef_df)] = ["Intercept", model.intercept_[0],
                               np.exp(model.intercept_[0]), ""]

print(coef_df.to_string(index=False))</code></pre>

                        <ul>
                            <li>A <strong>positive coefficient</strong> means that as the feature increases, the log-odds (and therefore the probability) of the positive class increase.</li>
                            <li>An <strong>odds ratio &gt; 1</strong> means the odds of the positive class increase with the feature; an odds ratio &lt; 1 means they decrease.</li>
                            <li>An odds ratio of 2.5 means: for a one-unit increase in the feature, the odds of the positive class multiply by 2.5, holding other features constant.</li>
                        </ul>

                        <blockquote>
                            <strong>Important:</strong> Coefficient interpretation assumes features are on comparable scales. If features are not standardised, the magnitude of coefficients is not directly comparable across features.
                        </blockquote>

                        <h2>Choosing the Decision Threshold</h2>

                        <p>By default, Scikit-Learn uses a threshold of 0.5. But the optimal threshold depends on the relative costs of false positives and false negatives:</p>

<pre><code class="language-python">from sklearn.metrics import precision_recall_curve

precision, recall, thresholds = precision_recall_curve(y_test, y_prob)

# Find threshold that maximises F1
f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)
best_idx = np.argmax(f1_scores)
best_threshold = thresholds[best_idx]

print(f"Best threshold (max F1): {best_threshold:.3f}")
print(f"Precision: {precision[best_idx]:.3f}")
print(f"Recall:    {recall[best_idx]:.3f}")
print(f"F1:        {f1_scores[best_idx]:.3f}")

# Plot precision-recall tradeoff
plt.figure(figsize=(8, 5))
plt.plot(thresholds, precision[:-1], label="Precision")
plt.plot(thresholds, recall[:-1], label="Recall")
plt.plot(thresholds, f1_scores[:-1], label="F1", linestyle="--")
plt.axvline(best_threshold, color="gray", linestyle=":", label=f"Best threshold={best_threshold:.2f}")
plt.xlabel("Decision Threshold")
plt.ylabel("Score")
plt.title("Precision, Recall, F1 vs Decision Threshold")
plt.legend()
plt.tight_layout()
plt.show()</code></pre>

                        <p>In medical screening, you might lower the threshold to increase recall (catch more positive cases). In spam detection, you might raise it to increase precision (fewer false alarms).</p>

                        <h2>Class Imbalance and Class Weights</h2>

                        <p>When one class is much rarer than the other, a naive model may simply predict the majority class. Logistic Regression offers the <code>class_weight</code> parameter to address this:</p>

<pre><code class="language-python"># Create imbalanced dataset
X_imb, y_imb = make_classification(
    n_samples=2000, n_features=10, n_redundant=2,
    n_informative=5, weights=[0.9, 0.1], random_state=42,
)

X_train_i, X_test_i, y_train_i, y_test_i = train_test_split(
    X_imb, y_imb, test_size=0.2, random_state=42
)

# Without class weights
model_no_weight = LogisticRegression(random_state=42)
model_no_weight.fit(X_train_i, y_train_i)

# With balanced class weights
model_balanced = LogisticRegression(class_weight="balanced", random_state=42)
model_balanced.fit(X_train_i, y_train_i)

print("Without class_weight:")
print(classification_report(y_test_i, model_no_weight.predict(X_test_i)))

print("\nWith class_weight='balanced':")
print(classification_report(y_test_i, model_balanced.predict(X_test_i)))</code></pre>

                        <p>Setting <code>class_weight="balanced"</code> automatically adjusts weights inversely proportional to class frequencies. This increases the penalty for misclassifying the minority class, typically improving recall at the cost of some precision.</p>

                        <h2>Regularization: L2, L1, and Elastic Net</h2>

                        <p>Scikit-Learn&rsquo;s <code>LogisticRegression</code> applies L2 regularisation by default (controlled by the <code>C</code> parameter, where smaller <code>C</code> means stronger regularisation). You can switch the penalty type:</p>

<pre><code class="language-python">from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# L2 regularisation (default) - Ridge-like
model_l2 = Pipeline([
    ("scaler", StandardScaler()),
    ("lr", LogisticRegression(penalty="l2", C=1.0, solver="lbfgs", random_state=42)),
])

# L1 regularisation - Lasso-like (sparse coefficients)
model_l1 = Pipeline([
    ("scaler", StandardScaler()),
    ("lr", LogisticRegression(penalty="l1", C=1.0, solver="saga", random_state=42)),
])

# Elastic Net (mix of L1 and L2)
model_en = Pipeline([
    ("scaler", StandardScaler()),
    ("lr", LogisticRegression(
        penalty="elasticnet", C=1.0, solver="saga",
        l1_ratio=0.5, random_state=42, max_iter=5000
    )),
])

for name, m in [("L2", model_l2), ("L1", model_l1), ("Elastic Net", model_en)]:
    m.fit(X_train, y_train)
    acc = accuracy_score(y_test, m.predict(X_test))
    n_nonzero = np.sum(m.named_steps["lr"].coef_ != 0)
    print(f"{name:12s} | Accuracy: {acc:.4f} | Non-zero coefficients: {n_nonzero}")</code></pre>

                        <ul>
                            <li><strong>L2 (Ridge):</strong> Shrinks all coefficients toward zero but rarely sets them exactly to zero. Good default.</li>
                            <li><strong>L1 (Lasso):</strong> Can set some coefficients exactly to zero, performing automatic feature selection.</li>
                            <li><strong>Elastic Net:</strong> Combines L1 and L2. Useful when features are correlated&mdash;L1 alone might arbitrarily pick one of several correlated features.</li>
                        </ul>

                        <p>The regularisation strength <code>C</code> should be tuned via cross-validation. Higher <code>C</code> means less regularisation (more complex model).</p>

<pre><code class="language-python">from sklearn.model_selection import GridSearchCV

param_grid = {
    "lr__C": np.logspace(-4, 4, 20),
}

grid = GridSearchCV(
    model_l2, param_grid, cv=5, scoring="accuracy", n_jobs=-1
)
grid.fit(X_train, y_train)

print(f"Best C: {grid.best_params_['lr__C']:.4f}")
print(f"Best CV Accuracy: {grid.best_score_:.4f}")
print(f"Test Accuracy:    {grid.score(X_test, y_test):.4f}")</code></pre>

                        <h2>Multiclass Logistic Regression</h2>

                        <p>Logistic Regression naturally extends to multiple classes. Scikit-Learn supports two strategies:</p>

                        <ul>
                            <li><strong>One-vs-Rest (OvR):</strong> Trains one binary classifier per class. The class with the highest confidence wins. Set <code>multi_class="ovr"</code>.</li>
                            <li><strong>Multinomial (Softmax):</strong> Fits a single model that optimises the cross-entropy loss across all classes simultaneously. Set <code>multi_class="multinomial"</code>. This is the default when the solver supports it.</li>
                        </ul>

<pre><code class="language-python">from sklearn.datasets import load_iris

iris = load_iris()
X_iris, y_iris = iris.data, iris.target

X_tr, X_te, y_tr, y_te = train_test_split(
    X_iris, y_iris, test_size=0.3, random_state=42
)

# Multinomial logistic regression
model_multi = Pipeline([
    ("scaler", StandardScaler()),
    ("lr", LogisticRegression(
        multi_class="multinomial", solver="lbfgs",
        max_iter=1000, random_state=42
    )),
])

model_multi.fit(X_tr, y_tr)
y_pred_iris = model_multi.predict(X_te)

print(f"Accuracy: {accuracy_score(y_te, y_pred_iris):.4f}")
print("\nClassification Report:")
print(classification_report(y_te, y_pred_iris, target_names=iris.target_names))

# Inspect probabilities for the first 5 samples
probs = model_multi.predict_proba(X_te[:5])
prob_df = pd.DataFrame(probs, columns=iris.target_names)
print("\nPredicted probabilities (first 5 samples):")
print(prob_df.to_string(index=False))</code></pre>

                        <h2>Statistical Inference with Statsmodels</h2>

                        <p>While Scikit-Learn is optimised for prediction, <code>statsmodels</code> provides the full statistical output that practitioners expect from logistic regression&mdash;p-values, confidence intervals, and likelihood-ratio tests:</p>

<pre><code class="language-python">import statsmodels.api as sm

# Add intercept manually (statsmodels does not add it by default)
X_train_sm = sm.add_constant(X_train)
X_test_sm = sm.add_constant(X_test)

logit_model = sm.Logit(y_train, X_train_sm)
result = logit_model.fit(disp=0)

print(result.summary())

# Odds ratios with 95% confidence intervals
params = result.params
conf = result.conf_int()
conf["OR"] = params
conf.columns = ["2.5%", "97.5%", "OR"]
conf = np.exp(conf)
print("\nOdds Ratios with 95% CI:")
print(conf)</code></pre>

                        <p>Key outputs to examine:</p>
                        <ul>
                            <li><strong>p-values:</strong> Test whether each coefficient is statistically different from zero.</li>
                            <li><strong>Pseudo R&sup2;:</strong> McFadden&rsquo;s R&sup2; gives a rough sense of model fit (higher is better, but values are typically much lower than in linear regression).</li>
                            <li><strong>AIC / BIC:</strong> Useful for model comparison; lower values indicate a better trade-off between fit and complexity.</li>
                        </ul>

                        <h2>Probability Calibration</h2>

                        <p>Logistic Regression generally produces well-calibrated probabilities out of the box. However, when using strong regularisation or class weights, calibration can degrade. Use a calibration curve to check:</p>

<pre><code class="language-python">from sklearn.calibration import calibration_curve, CalibratedClassifierCV

prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)

plt.figure(figsize=(7, 7))
plt.plot(prob_pred, prob_true, "s-", label="Logistic Regression")
plt.plot([0, 1], [0, 1], "k--", label="Perfectly calibrated")
plt.xlabel("Mean predicted probability")
plt.ylabel("Fraction of positives")
plt.title("Calibration Curve (Reliability Diagram)")
plt.legend()
plt.tight_layout()
plt.show()</code></pre>

                        <p>If calibration is poor, apply <code>CalibratedClassifierCV</code> with either isotonic regression or Platt scaling:</p>

<pre><code class="language-python"># Re-calibrate with isotonic regression
calibrated = CalibratedClassifierCV(model, method="isotonic", cv=5)
calibrated.fit(X_train, y_train)
y_prob_cal = calibrated.predict_proba(X_test)[:, 1]

prob_true_cal, prob_pred_cal = calibration_curve(y_test, y_prob_cal, n_bins=10)

plt.figure(figsize=(7, 7))
plt.plot(prob_pred, prob_true, "s-", label="Before calibration")
plt.plot(prob_pred_cal, prob_true_cal, "o-", label="After calibration")
plt.plot([0, 1], [0, 1], "k--", label="Perfect")
plt.xlabel("Mean predicted probability")
plt.ylabel("Fraction of positives")
plt.title("Calibration: Before vs After")
plt.legend()
plt.tight_layout()
plt.show()</code></pre>

                        <h2>Pipelines with Categorical Features</h2>

                        <p>Real-world datasets typically include categorical variables. Here is a complete pipeline that handles mixed data types:</p>

<pre><code class="language-python">from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder

# Example with a mixed-type dataset
# Using fetch_openml for the Titanic dataset as an example
from sklearn.datasets import fetch_openml

titanic = fetch_openml("titanic", version=1, as_frame=True, parser="auto")
df = titanic.frame.dropna(subset=["survived"])

# Select features
features_to_use = ["pclass", "sex", "age", "sibsp", "parch", "fare", "embarked"]
df = df[features_to_use + ["survived"]].dropna()

target = "survived"
X_tit = df[features_to_use]
y_tit = df[target].astype(int)

cat_features = ["sex", "embarked", "pclass"]
num_features = ["age", "sibsp", "parch", "fare"]

preprocessor = ColumnTransformer([
    ("num", StandardScaler(), num_features),
    ("cat", OneHotEncoder(drop="first", sparse_output=False), cat_features),
])

pipe_tit = Pipeline([
    ("prep", preprocessor),
    ("lr", LogisticRegression(max_iter=1000, random_state=42)),
])

X_tr_t, X_te_t, y_tr_t, y_te_t = train_test_split(
    X_tit, y_tit, test_size=0.2, random_state=42
)

pipe_tit.fit(X_tr_t, y_tr_t)
y_pred_t = pipe_tit.predict(X_te_t)

print(f"Accuracy: {accuracy_score(y_te_t, y_pred_t):.4f}")
print(classification_report(y_te_t, y_pred_t))</code></pre>

                        <blockquote>
                            <strong>Tip:</strong> Use <code>OneHotEncoder(drop="first")</code> to avoid the dummy variable trap (multicollinearity). For logistic regression, this also makes coefficients more interpretable since each coefficient represents the effect relative to the dropped category.
                        </blockquote>

                        <h2>Diagnostics and Common Pitfalls</h2>

                        <h3>1. Multicollinearity</h3>
                        <p>Highly correlated features inflate the variance of coefficient estimates, making them unstable and hard to interpret. Check the Variance Inflation Factor (VIF):</p>

<pre><code class="language-python">from statsmodels.stats.outliers_influence import variance_inflation_factor

X_check = sm.add_constant(X_train)
vif_data = pd.DataFrame()
vif_data["Feature"] = ["const"] + [f"X{i}" for i in range(X_train.shape[1])]
vif_data["VIF"] = [
    variance_inflation_factor(X_check, i) for i in range(X_check.shape[1])
]
print(vif_data)</code></pre>

                        <p>A VIF above 5&ndash;10 suggests problematic collinearity. Remedies include removing correlated features, using PCA, or applying L1/L2 regularisation.</p>

                        <h3>2. Linearity in the Log-Odds</h3>
                        <p>Logistic Regression assumes a linear relationship between features and the log-odds. If the true relationship is nonlinear, the model will underfit. Add polynomial or interaction features, or switch to a nonlinear model.</p>

                        <h3>3. Separability</h3>
                        <p>When one or more features can perfectly separate the classes, the maximum-likelihood estimate does not exist (coefficients diverge to infinity). Regularisation (any <code>C &lt; &infin;</code>) solves this problem.</p>

                        <h3>4. Sample Size</h3>
                        <p>A common rule of thumb is to have at least 10&ndash;20 events (minority class observations) per feature. With too few events per predictor, the model becomes unreliable.</p>

                        <h3>5. Ignoring Feature Scaling</h3>
                        <p>Logistic Regression with regularisation is sensitive to feature scales. Always standardise or normalise features before fitting, especially when comparing coefficient magnitudes.</p>

                        <h2>Feature Engineering with PolynomialFeatures</h2>

                        <p>When the decision boundary is not linear, adding polynomial and interaction terms can help:</p>

<pre><code class="language-python">from sklearn.preprocessing import PolynomialFeatures

# Create nonlinear dataset
from sklearn.datasets import make_moons
X_moon, y_moon = make_moons(n_samples=500, noise=0.2, random_state=42)
X_tr_m, X_te_m, y_tr_m, y_te_m = train_test_split(
    X_moon, y_moon, test_size=0.2, random_state=42
)

# Linear logistic regression
pipe_linear = Pipeline([
    ("scaler", StandardScaler()),
    ("lr", LogisticRegression(random_state=42)),
])

# Polynomial logistic regression
pipe_poly = Pipeline([
    ("poly", PolynomialFeatures(degree=3, include_bias=False)),
    ("scaler", StandardScaler()),
    ("lr", LogisticRegression(C=10, random_state=42, max_iter=1000)),
])

pipe_linear.fit(X_tr_m, y_tr_m)
pipe_poly.fit(X_tr_m, y_tr_m)

print(f"Linear accuracy:     {pipe_linear.score(X_te_m, y_te_m):.4f}")
print(f"Polynomial accuracy: {pipe_poly.score(X_te_m, y_te_m):.4f}")

# Visualise both decision boundaries
fig, axes = plt.subplots(1, 2, figsize=(14, 5))
for ax, pipe_m, title in [
    (axes[0], pipe_linear, "Linear"),
    (axes[1], pipe_poly, "Polynomial (degree=3)")
]:
    h = 0.02
    x_min, x_max = X_moon[:, 0].min() - 0.5, X_moon[:, 0].max() + 0.5
    y_min, y_max = X_moon[:, 1].min() - 0.5, X_moon[:, 1].max() + 0.5
    xx, yy = np.meshgrid(
        np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)
    )
    Z = pipe_m.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
    ax.contourf(xx, yy, Z, alpha=0.4, cmap="RdBu_r")
    ax.scatter(X_te_m[:, 0], X_te_m[:, 1], c=y_te_m, cmap="RdBu_r",
               edgecolors="k", s=30)
    ax.set_title(title)

plt.tight_layout()
plt.show()</code></pre>

                        <p>Adding polynomial features transforms a linear classifier into one capable of learning curved decision boundaries. However, the number of features grows combinatorially with degree, so regularisation becomes essential.</p>

                        <h2>End-to-End Template</h2>

                        <p>Copy and adapt this template for your own logistic regression projects:</p>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    accuracy_score, classification_report,
    ConfusionMatrixDisplay, RocCurveDisplay
)
from sklearn.calibration import calibration_curve

# ── 1. Load data ───────────────────────────────────────────
X, y = make_classification(
    n_samples=2000, n_features=10, n_informative=6,
    n_redundant=2, random_state=42,
)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ── 2. Pipeline ────────────────────────────────────────────
pipe = Pipeline([
    ("scaler", StandardScaler()),
    ("lr", LogisticRegression(
        penalty="l2", solver="lbfgs",
        max_iter=1000, random_state=42
    )),
])

# ── 3. Hyperparameter search ──────────────────────────────
param_grid = {
    "lr__C": np.logspace(-4, 4, 20),
}

grid = GridSearchCV(
    pipe, param_grid, cv=5,
    scoring="accuracy", n_jobs=-1,
)
grid.fit(X_train, y_train)

print("Best C:", grid.best_params_["lr__C"])
print("Best CV Accuracy:", grid.best_score_)

# ── 4. Evaluate on test set ───────────────────────────────
y_pred = grid.predict(X_test)
y_prob = grid.predict_proba(X_test)[:, 1]

print(f"\nTest Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print(classification_report(y_test, y_pred))

# ── 5. Diagnostics ────────────────────────────────────────
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Confusion matrix
ConfusionMatrixDisplay.from_predictions(y_test, y_pred, ax=axes[0])
axes[0].set_title("Confusion Matrix")

# ROC curve
RocCurveDisplay.from_predictions(y_test, y_prob, ax=axes[1])
axes[1].set_title("ROC Curve")

# Calibration curve
prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)
axes[2].plot(prob_pred, prob_true, "s-", label="Model")
axes[2].plot([0, 1], [0, 1], "k--", label="Perfect")
axes[2].set_xlabel("Mean predicted probability")
axes[2].set_ylabel("Fraction of positives")
axes[2].set_title("Calibration Curve")
axes[2].legend()

plt.tight_layout()
plt.show()

# ── 6. Coefficients ───────────────────────────────────────
best_lr = grid.best_estimator_.named_steps["lr"]
coef_df = pd.DataFrame({
    "Feature": [f"X{i}" for i in range(X.shape[1])],
    "Coefficient": best_lr.coef_[0],
    "Odds Ratio": np.exp(best_lr.coef_[0]),
})
print("\nCoefficients and Odds Ratios:")
print(coef_df.sort_values("Coefficient", ascending=False).to_string(index=False))</code></pre>

                        <h2>Key Takeaways</h2>

                        <ul>
                            <li>Logistic Regression models the log-odds as a linear function of features, producing calibrated probabilities via the sigmoid function.</li>
                            <li>Coefficients are directly interpretable: exponentiated coefficients give odds ratios.</li>
                            <li>The decision threshold (default 0.5) should be tuned based on the cost of false positives vs false negatives.</li>
                            <li>Use <code>class_weight="balanced"</code> for imbalanced datasets.</li>
                            <li>L2 regularisation is the default; use L1 for feature selection, Elastic Net when features are correlated.</li>
                            <li>Always scale features before fitting, especially with regularisation.</li>
                            <li>Use <code>statsmodels</code> for statistical inference (p-values, confidence intervals).</li>
                            <li>Check calibration with reliability diagrams; recalibrate if needed.</li>
                            <li>Add <code>PolynomialFeatures</code> when the decision boundary is nonlinear.</li>
                            <li>Logistic Regression is fast, interpretable, and often surprisingly competitive&mdash;always try it as a baseline before reaching for complex models.</li>
                        </ul>

                    </div>
                </div>

                <aside class="article-sidebar">
                    <div class="sidebar-author">
                        <img src="images/pm.png" alt="Paolo Mascia" class="sidebar-author-photo">
                        <div class="sidebar-author-name">Paolo Mascia</div>
                        <div class="sidebar-author-role">AI &amp; Cloud Architect</div>
                        <p class="sidebar-author-bio">25+ years designing large-scale distributed systems. Specialized in Generative AI, AI Agents, and cloud-native architectures.</p>
                        <div class="sidebar-author-links">
                            <a href="https://www.linkedin.com/in/paolo-mascia-italy" target="_blank" aria-label="LinkedIn">
                                <svg viewBox="0 0 24 24"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 01-2.063-2.065 2.064 2.064 0 112.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/></svg>
                            </a>
                            <a href="https://github.com/paolomascia" target="_blank" aria-label="GitHub">
                                <svg viewBox="0 0 24 24"><path d="M12 0C5.374 0 0 5.373 0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23A11.509 11.509 0 0112 5.803c1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
                            </a>
                            <a href="https://www.kaggle.com/paolomascia" target="_blank" aria-label="Kaggle">
                                <svg viewBox="0 0 24 24"><path d="M18.825 23.859c-.022.092-.117.141-.281.141h-3.139c-.187 0-.351-.082-.492-.248l-5.178-6.589-1.448 1.374v5.111c0 .235-.117.352-.351.352H5.505c-.236 0-.354-.117-.354-.352V.353c0-.233.118-.353.354-.353h2.431c.234 0 .351.12.351.353v14.343l6.203-6.272c.165-.165.33-.246.495-.246h3.239c.144 0 .236.06.281.18.046.149.034.233-.036.315l-6.555 6.344 6.836 8.507c.095.104.117.208.075.378z"/></svg>
                            </a>
                        </div>
                    </div>
                    <div class="sidebar-box">
                        <h4>More Articles</h4>
                        <ul>
                            <li><a href="post-svm.html">Support Vector Machines in Python</a><span class="sidebar-post-meta">SVM &middot; Oct 2024</span></li>
                            <li><a href="post-simple-linear-regression.html">Simple Linear Regression</a><span class="sidebar-post-meta">Linear Regression &middot; Feb 2023</span></li>
                            <li><a href="post-regularization.html">Regularization: Ridge, Lasso, Elastic Net</a><span class="sidebar-post-meta">Regularization &middot; Jul 2023</span></li>
                            <li><a href="post-knn.html">K-Nearest Neighbors Regression</a><span class="sidebar-post-meta">KNN &middot; Jun 2024</span></li>
                            <li><a href="post-random-forest.html">Random Forest Model Evaluation</a><span class="sidebar-post-meta">Model Evaluation &middot; Dec 2023</span></li>
                        </ul>
                    </div>
                </aside>
            </div>
        </div>
    </article>

    <footer class="footer">
        <div class="container">
            <p>&copy; 2025 Paolo Mascia. Built with curiosity and too much coffee.</p>
        </div>
    </footer>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
</body>
</html>