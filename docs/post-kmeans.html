<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>K-Means Clustering Model Evaluation in Python &mdash; Paolo Mascia</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="article.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
</head>
<body>

    <nav class="navbar scrolled">
        <div class="container">
            <a href="index.html" class="nav-logo">
                <span class="logo-accent">&gt;</span> paolo.mascia
            </a>
            <div class="nav-links">
                <a href="index.html#about">About</a>
                <a href="index.html#projects">Projects</a>
                <a href="ai-tech-insights.html">AI Tech Insights</a>
                <a href="cybersecurity-tech-insights.html">Cybersecurity</a>
                <a href="golang-tech-insights.html">Go</a>
                <a href="index.html#contact">Contact</a>
            </div>
        </div>
    </nav>

    <article class="article">
        <div class="container">
            <div class="article-layout">
                <div class="article-main">
                    <header class="article-header">
                        <a href="ai-tech-insights.html" class="back-link">&larr; Back to Tech Insights</a>
                        <div class="post-meta">
                            <span class="post-date">Feb 2024</span>
                            <span class="post-reading">12 min read</span>
                        </div>
                        <h1>K-Means Clustering Model Evaluation in Python</h1>
                        <div class="post-tags">
                            <span>K-Means</span>
                            <span>Clustering</span>
                            <span>Model Evaluation</span>
                        </div>
                    </header>

                    <div class="article-body">
                        <p class="lead">K-Means is one of the most popular unsupervised clustering algorithms. Since clustering lacks ground truth labels, evaluation requires internal and external validation metrics that measure cluster quality, cohesion, and separation.</p>

                        <h2>Why Evaluation Matters in Clustering</h2>
                        <p>Unlike supervised learning where accuracy or F1 score provide clear feedback, clustering operates without labeled data. Without proper evaluation, you cannot determine whether your clusters are meaningful or simply artifacts of the algorithm&rsquo;s assumptions.</p>
                        <p>Clustering evaluation answers critical questions:</p>
                        <ul>
                            <li><strong>How many clusters should we use?</strong></li>
                            <li><strong>Are the clusters well-separated?</strong></li>
                            <li><strong>Are data points tightly grouped within each cluster?</strong></li>
                            <li><strong>Would a different algorithm produce better results?</strong></li>
                        </ul>

                        <h2>Setup and Clustering</h2>
                        <p>We generate a synthetic dataset with known cluster structure to demonstrate each evaluation method. This lets us compare internal metrics (no labels needed) with external metrics (using the true labels).</p>
                        <pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Generate synthetic data with 4 true clusters
X, y_true = make_blobs(n_samples=500, centers=4, cluster_std=1.0, random_state=42)

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Fit K-Means with k=4
kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
labels = kmeans.fit_predict(X_scaled)

# Visualize the clusters
plt.figure(figsize=(8, 5))
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels, cmap="viridis", s=30, alpha=0.7)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
            c="red", marker="X", s=200, edgecolors="black", label="Centroids")
plt.title("K-Means Clustering (k=4)")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.legend()
plt.tight_layout()
plt.savefig("kmeans_clusters.png", dpi=150)
plt.show()</code></pre>

                        <h2>Internal Validation Metrics</h2>
                        <p>Internal metrics evaluate clustering quality using only the data and cluster assignments &mdash; no ground truth labels required.</p>

                        <h3>Silhouette Score</h3>
                        <p>Measures how similar each point is to its own cluster versus the nearest neighboring cluster. Values range from &minus;1 to 1, where higher is better.</p>

                        <h3>Davies-Bouldin Index</h3>
                        <p>Computes the average similarity between each cluster and the one most similar to it. Lower values indicate better-defined clusters.</p>

                        <h3>Calinski-Harabasz Index</h3>
                        <p>The ratio of between-cluster dispersion to within-cluster dispersion. Higher values indicate denser, well-separated clusters.</p>

                        <pre><code class="language-python">from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

sil = silhouette_score(X_scaled, labels)
dbi = davies_bouldin_score(X_scaled, labels)
chi = calinski_harabasz_score(X_scaled, labels)

print(f"Silhouette Score:       {sil:.4f}   (higher is better, range [-1, 1])")
print(f"Davies-Bouldin Index:   {dbi:.4f}   (lower is better)")
print(f"Calinski-Harabasz Index: {chi:.2f}  (higher is better)")</code></pre>

                        <h2>Comparing K Values with Metrics</h2>
                        <p>To find the optimal number of clusters, compute internal metrics across a range of k values and look for peaks or elbows.</p>
                        <pre><code class="language-python">k_range = range(2, 11)
sil_scores = []
dbi_scores = []
chi_scores = []

for k in k_range:
    km = KMeans(n_clusters=k, random_state=42, n_init=10)
    preds = km.fit_predict(X_scaled)
    sil_scores.append(silhouette_score(X_scaled, preds))
    dbi_scores.append(davies_bouldin_score(X_scaled, preds))
    chi_scores.append(calinski_harabasz_score(X_scaled, preds))

fig, axes = plt.subplots(1, 3, figsize=(16, 4))

axes[0].plot(k_range, sil_scores, "bo-")
axes[0].set_title("Silhouette Score")
axes[0].set_xlabel("k")
axes[0].set_ylabel("Score")

axes[1].plot(k_range, dbi_scores, "ro-")
axes[1].set_title("Davies-Bouldin Index")
axes[1].set_xlabel("k")
axes[1].set_ylabel("Score")

axes[2].plot(k_range, chi_scores, "go-")
axes[2].set_title("Calinski-Harabasz Index")
axes[2].set_xlabel("k")
axes[2].set_ylabel("Score")

plt.tight_layout()
plt.savefig("kmeans_metrics_comparison.png", dpi=150)
plt.show()</code></pre>

                        <h2>Elbow Method</h2>
                        <p>The Elbow Method plots inertia (within-cluster sum of squares) against k. The &ldquo;elbow&rdquo; point &mdash; where the rate of decrease sharply changes &mdash; suggests the optimal number of clusters.</p>
                        <pre><code class="language-python">inertias = []

for k in k_range:
    km = KMeans(n_clusters=k, random_state=42, n_init=10)
    km.fit(X_scaled)
    inertias.append(km.inertia_)

plt.figure(figsize=(8, 5))
plt.plot(k_range, inertias, "bo-", linewidth=2)
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Inertia (WCSS)")
plt.title("Elbow Method")
plt.xticks(list(k_range))
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig("kmeans_elbow.png", dpi=150)
plt.show()</code></pre>

                        <h2>External Validation</h2>
                        <p>When ground truth labels are available (as in our synthetic dataset), external metrics measure how well the clustering matches the true structure.</p>
                        <pre><code class="language-python">from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score
from sklearn.metrics import homogeneity_score, completeness_score

ari = adjusted_rand_score(y_true, labels)
nmi = normalized_mutual_info_score(y_true, labels)
homo = homogeneity_score(y_true, labels)
comp = completeness_score(y_true, labels)

print(f"Adjusted Rand Index (ARI):       {ari:.4f}   (1.0 = perfect)")
print(f"Normalized Mutual Information:    {nmi:.4f}   (1.0 = perfect)")
print(f"Homogeneity:                      {homo:.4f}  (1.0 = perfect)")
print(f"Completeness:                     {comp:.4f}  (1.0 = perfect)")</code></pre>

                        <h2>Cluster Stability Analysis</h2>
                        <p>A stable clustering should produce similar results across different random subsamples. We can measure stability by repeatedly clustering bootstrapped samples and comparing the results.</p>
                        <pre><code class="language-python">from sklearn.metrics import adjusted_rand_score

n_runs = 20
ari_scores = []

for i in range(n_runs):
    # Bootstrap sample
    indices = np.random.choice(len(X_scaled), size=len(X_scaled), replace=True)
    X_boot = X_scaled[indices]

    km_boot = KMeans(n_clusters=4, random_state=i, n_init=10)
    labels_boot = km_boot.fit_predict(X_boot)

    # Compare with full-data labels for the sampled indices
    ari_boot = adjusted_rand_score(labels[indices], labels_boot)
    ari_scores.append(ari_boot)

print(f"Stability ARI:  {np.mean(ari_scores):.4f} +/- {np.std(ari_scores):.4f}")
print(f"Min ARI: {np.min(ari_scores):.4f},  Max ARI: {np.max(ari_scores):.4f}")</code></pre>

                        <h2>Visualizing Silhouette Analysis</h2>
                        <p>A silhouette plot shows the silhouette coefficient for each sample, grouped by cluster. This reveals whether individual clusters are cohesive and well-separated.</p>
                        <pre><code class="language-python">from sklearn.metrics import silhouette_samples

sil_vals = silhouette_samples(X_scaled, labels)
n_clusters = len(np.unique(labels))

fig, ax = plt.subplots(figsize=(8, 6))
y_lower = 10

for i in range(n_clusters):
    cluster_sil = np.sort(sil_vals[labels == i])
    size_cluster = cluster_sil.shape[0]
    y_upper = y_lower + size_cluster

    ax.fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_sil, alpha=0.7)
    ax.text(-0.05, y_lower + 0.5 * size_cluster, str(i), fontweight="bold")
    y_lower = y_upper + 10

ax.axvline(x=sil, color="red", linestyle="--", label=f"Mean = {sil:.3f}")
ax.set_xlabel("Silhouette Coefficient")
ax.set_ylabel("Cluster")
ax.set_title("Silhouette Analysis (k=4)")
ax.legend()
plt.tight_layout()
plt.savefig("kmeans_silhouette.png", dpi=150)
plt.show()</code></pre>

                        <h2>Summary of Metrics</h2>
                        <table>
                            <thead>
                                <tr>
                                    <th>Metric</th>
                                    <th>Type</th>
                                    <th>Range</th>
                                    <th>Goal</th>
                                    <th>Labels Needed?</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Silhouette Score</td>
                                    <td>Internal</td>
                                    <td>[&minus;1, 1]</td>
                                    <td>Higher is better</td>
                                    <td>No</td>
                                </tr>
                                <tr>
                                    <td>Davies-Bouldin Index</td>
                                    <td>Internal</td>
                                    <td>[0, &infin;)</td>
                                    <td>Lower is better</td>
                                    <td>No</td>
                                </tr>
                                <tr>
                                    <td>Calinski-Harabasz</td>
                                    <td>Internal</td>
                                    <td>[0, &infin;)</td>
                                    <td>Higher is better</td>
                                    <td>No</td>
                                </tr>
                                <tr>
                                    <td>Inertia (WCSS)</td>
                                    <td>Internal</td>
                                    <td>[0, &infin;)</td>
                                    <td>Lower is better</td>
                                    <td>No</td>
                                </tr>
                                <tr>
                                    <td>Adjusted Rand Index</td>
                                    <td>External</td>
                                    <td>[&minus;1, 1]</td>
                                    <td>Higher is better</td>
                                    <td>Yes</td>
                                </tr>
                                <tr>
                                    <td>Normalized Mutual Info</td>
                                    <td>External</td>
                                    <td>[0, 1]</td>
                                    <td>Higher is better</td>
                                    <td>Yes</td>
                                </tr>
                                <tr>
                                    <td>Homogeneity</td>
                                    <td>External</td>
                                    <td>[0, 1]</td>
                                    <td>Higher is better</td>
                                    <td>Yes</td>
                                </tr>
                                <tr>
                                    <td>Completeness</td>
                                    <td>External</td>
                                    <td>[0, 1]</td>
                                    <td>Higher is better</td>
                                    <td>Yes</td>
                                </tr>
                            </tbody>
                        </table>

                        <h2>Key Takeaways</h2>
                        <ul>
                            <li>Always evaluate clustering results &mdash; K-Means will produce clusters regardless of whether the data has natural groupings.</li>
                            <li>Use internal metrics (Silhouette, Davies-Bouldin, Calinski-Harabasz) when no ground truth labels exist.</li>
                            <li>The Elbow Method provides a quick visual heuristic for selecting k, but should be combined with other metrics.</li>
                            <li>External metrics (ARI, NMI) are the gold standard when true labels are available for validation.</li>
                            <li>Stability analysis via bootstrapping reveals whether your clustering is robust or sensitive to data perturbations.</li>
                            <li>Silhouette plots give per-cluster insight, exposing weak or overlapping clusters that aggregate scores may hide.</li>
                        </ul>
                    </div>
                </div>

                <aside class="article-sidebar">
                    <div class="sidebar-author">
                        <img src="images/pm.png" alt="Paolo Mascia" class="sidebar-author-photo">
                        <div class="sidebar-author-name">Paolo Mascia</div>
                        <div class="sidebar-author-role">AI &amp; Cloud Architect</div>
                        <p class="sidebar-author-bio">25+ years designing large-scale distributed systems. Specialized in Generative AI, AI Agents, and cloud-native architectures.</p>
                        <div class="sidebar-author-links">
                            <a href="https://www.linkedin.com/in/paolo-mascia-italy" target="_blank" aria-label="LinkedIn">
                                <svg viewBox="0 0 24 24"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 01-2.063-2.065 2.064 2.064 0 112.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/></svg>
                            </a>
                            <a href="https://github.com/paolomascia" target="_blank" aria-label="GitHub">
                                <svg viewBox="0 0 24 24"><path d="M12 0C5.374 0 0 5.373 0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23A11.509 11.509 0 0112 5.803c1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
                            </a>
                            <a href="https://www.kaggle.com/paolomascia" target="_blank" aria-label="Kaggle">
                                <svg viewBox="0 0 24 24"><path d="M18.825 23.859c-.022.092-.117.141-.281.141h-3.139c-.187 0-.351-.082-.492-.248l-5.178-6.589-1.448 1.374v5.111c0 .235-.117.352-.351.352H5.505c-.236 0-.354-.117-.354-.352V.353c0-.233.118-.353.354-.353h2.431c.234 0 .351.12.351.353v14.343l6.203-6.272c.165-.165.33-.246.495-.246h3.239c.144 0 .236.06.281.18.046.149.034.233-.036.315l-6.555 6.344 6.836 8.507c.095.104.117.208.075.378z"/></svg>
                            </a>
                        </div>
                    </div>
                    <div class="sidebar-box">
                        <h4>More Articles</h4>
                        <a href="post-pca.html" class="sidebar-link">
                            <span class="sidebar-link-tag">PCA</span>
                            <div class="sidebar-link-title">Principal Component Analysis (PCA) in Python</div>
                            <span class="sidebar-link-meta">May 2024</span>
                        </a>
                        <a href="post-tsne-umap.html" class="sidebar-link">
                            <span class="sidebar-link-tag">Dimension Reduction</span>
                            <div class="sidebar-link-title">t-SNE and UMAP in Python</div>
                            <span class="sidebar-link-meta">Jul 2024</span>
                        </a>
                        <a href="post-random-forest.html" class="sidebar-link">
                            <span class="sidebar-link-tag">Model Evaluation</span>
                            <div class="sidebar-link-title">Random Forest Model Evaluation</div>
                            <span class="sidebar-link-meta">Dec 2023</span>
                        </a>
                        <a href="post-kmeans-clustering.html" class="sidebar-link">
                            <span class="sidebar-link-tag">Clustering</span>
                            <div class="sidebar-link-title">K-Means Clustering in Python</div>
                            <span class="sidebar-link-meta">Sep 2024</span>
                        </a>
                        <a href="post-dbscan.html" class="sidebar-link">
                            <span class="sidebar-link-tag">Clustering</span>
                            <div class="sidebar-link-title">DBSCAN and HDBSCAN in Python</div>
                            <span class="sidebar-link-meta">Nov 2024</span>
                        </a>
                    </div>
                </aside>
            </div>
        </div>
    </article>

    <footer class="footer">
        <div class="container">
            <p>&copy; 2025 Paolo Mascia. Built with curiosity and too much coffee.</p>
        </div>
    </footer>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
</body>
</html>