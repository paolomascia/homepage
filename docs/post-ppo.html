<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Proximal Policy Optimization (PPO): A Practical, Stable Workhorse for RL â€” Paolo Mascia</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="article.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
</head>
<body>

    <nav class="navbar scrolled">
        <div class="container">
            <a href="index.html" class="nav-logo">
                <span class="logo-accent">&gt;</span> paolo.mascia
            </a>
            <div class="nav-links">
                <a href="index.html#about">About</a>
                <a href="index.html#projects">Projects</a>
                <a href="ai-tech-insights.html">AI Tech Insights</a>
                <a href="cybersecurity-tech-insights.html">Cybersecurity</a>
                <a href="golang-tech-insights.html">Go</a>
                <a href="index.html#contact">Contact</a>
            </div>
        </div>
    </nav>

    <article class="article">
        <div class="container">
            <div class="article-layout">
                <div class="article-main">
                    <header class="article-header">
                        <a href="ai-tech-insights.html" class="back-link">&larr; Back to Tech Insights</a>
                        <div class="post-meta">
                            <span class="post-date">Oct 2025</span>
                            <span class="post-reading">10 min read</span>
                        </div>
                        <h1>Proximal Policy Optimization (PPO): A Practical, Stable Workhorse for RL</h1>
                        <div class="post-tags">
                            <span>Reinforcement Learning</span>
                            <span>PPO</span>
                            <span>RLHF</span>
                        </div>
                    </header>

                    <div class="article-body">

                        <p>Vanilla policy gradients can make large updates that collapse performance. PPO constrains policy changes to stay &ldquo;close&rdquo; to the old policy, so learning proceeds steadily. You still improve returns, but you avoid stepping too far and breaking what already works.</p>

                        <p>PPO was introduced by Schulman et al. (2017) at OpenAI and has since become the default policy optimization algorithm in both game environments and language model alignment (RLHF). Its popularity comes from a rare combination: <strong>simplicity, stability, and strong empirical performance</strong>.</p>

                        <h2>Where PPO Fits in the RL Landscape</h2>

                        <p>Reinforcement learning algorithms fall into two broad families:</p>

                        <ul>
                            <li><strong>Value-based methods</strong> (DQN, etc.) &mdash; learn a value function and derive the policy from it. Work well with discrete actions but struggle with continuous or very large action spaces.</li>
                            <li><strong>Policy gradient methods</strong> (REINFORCE, A2C, PPO, TRPO) &mdash; directly optimize the policy. Handle continuous and high-dimensional action spaces naturally.</li>
                        </ul>

                        <p>Within policy gradient methods, the challenge is controlling update magnitude:</p>

                        <table>
                            <thead>
                                <tr>
                                    <th>Method</th>
                                    <th>Update Control</th>
                                    <th>Trade-off</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>REINFORCE</strong></td>
                                    <td>None (raw gradient)</td>
                                    <td>High variance, unstable</td>
                                </tr>
                                <tr>
                                    <td><strong>A2C</strong></td>
                                    <td>Baseline subtraction</td>
                                    <td>Lower variance, but can still overshoot</td>
                                </tr>
                                <tr>
                                    <td><strong>TRPO</strong></td>
                                    <td>Hard KL constraint</td>
                                    <td>Stable but complex (requires conjugate gradient)</td>
                                </tr>
                                <tr>
                                    <td><strong>PPO</strong></td>
                                    <td>Clipped surrogate objective</td>
                                    <td>Stable, simple, fast &mdash; the practical sweet spot</td>
                                </tr>
                            </tbody>
                        </table>

                        <p>PPO achieves TRPO-level stability with first-order optimization (standard SGD/Adam) &mdash; no second-order methods needed.</p>

                        <h2>The Core Objective: Clipped Surrogate</h2>

                        <p>The central idea of PPO is the <strong>clipped surrogate objective</strong>. Define the probability ratio:</p>

<pre><code class="language-python">r_t(&theta;) = &pi;_&theta;(a_t | s_t) / &pi;_&theta;_old(a_t | s_t)
</code></pre>

                        <p>This ratio measures how much the new policy differs from the old one for a given state-action pair. The PPO-Clip objective is:</p>

<pre><code class="language-python">L_CLIP(&theta;) = E_t [ min(
    r_t(&theta;) * A_t,
    clip(r_t(&theta;), 1 - &epsilon;, 1 + &epsilon;) * A_t
)]
</code></pre>

                        <p>Where:</p>
                        <ul>
                            <li><strong>A<sub>t</sub></strong> is the advantage estimate at timestep <em>t</em> (how much better this action was than expected)</li>
                            <li><strong>&epsilon;</strong> is the clipping parameter (typically 0.1 or 0.2)</li>
                            <li>The <code>min</code> ensures that the objective is a <strong>lower bound</strong> &mdash; it prevents exploiting large ratio values</li>
                        </ul>

                        <p><strong>How it works intuitively:</strong></p>
                        <ul>
                            <li>If the advantage is <strong>positive</strong> (good action), we want to increase its probability &mdash; but not beyond <code>1 + &epsilon;</code>.</li>
                            <li>If the advantage is <strong>negative</strong> (bad action), we want to decrease its probability &mdash; but not below <code>1 - &epsilon;</code>.</li>
                            <li>Either way, the update is bounded, preventing catastrophic policy changes.</li>
                        </ul>

                        <h2>Common Variants</h2>

                        <h3>PPO-Clip (Default)</h3>

                        <p>This is the standard variant described above and the one used in virtually all practical implementations. It is simple, effective, and requires no second-order optimization.</p>

                        <h3>PPO-KL (Adaptive KL Penalty)</h3>

                        <p>Instead of clipping, add a KL divergence penalty to the loss:</p>

<pre><code class="language-python">L_KL(&theta;) = E_t [ r_t(&theta;) * A_t - &beta; * KL(&pi;_&theta;_old || &pi;_&theta;) ]
</code></pre>

                        <p>The coefficient &beta; is adapted dynamically: if KL exceeds a target, &beta; increases; if KL is too small, &beta; decreases. In practice, PPO-Clip tends to be simpler and equally effective, which is why it dominates.</p>

                        <h2>Key Ingredients</h2>

                        <p>A typical PPO implementation requires these components:</p>

                        <ol>
                            <li><strong>Policy network</strong> &mdash; outputs action probabilities (or a distribution) given a state.</li>
                            <li><strong>Value network</strong> &mdash; estimates the expected return from a given state (used for advantage estimation).</li>
                            <li><strong>Advantage estimator</strong> &mdash; typically Generalized Advantage Estimation (GAE), which balances bias and variance.</li>
                            <li><strong>Rollout buffer</strong> &mdash; stores trajectories collected under the current policy.</li>
                            <li><strong>Multiple epochs of updates</strong> &mdash; PPO reuses each batch of experience for several gradient steps (since clipping keeps updates safe).</li>
                        </ol>

                        <h2>One PPO Update Cycle</h2>

                        <p>Here is the step-by-step flow of a single PPO iteration:</p>

                        <ol>
                            <li><strong>Collect rollouts:</strong> Run the current policy in the environment for <em>T</em> timesteps. Store (state, action, reward, log_prob, value) for each step.</li>
                            <li><strong>Compute returns and advantages:</strong> Use GAE to compute advantage estimates A<sub>t</sub> and discounted returns R<sub>t</sub>.</li>
                            <li><strong>Normalize advantages:</strong> Subtract mean, divide by standard deviation (batch-level normalization).</li>
                            <li><strong>For each epoch (K epochs):</strong>
                                <ul>
                                    <li>Shuffle the rollout data into mini-batches.</li>
                                    <li>For each mini-batch, compute the clipped policy loss, value loss, and entropy bonus.</li>
                                    <li>Backpropagate and update with Adam.</li>
                                </ul>
                            </li>
                            <li><strong>Replace old policy:</strong> Set &pi;<sub>old</sub> = &pi;<sub>&theta;</sub> and repeat from step 1.</li>
                        </ol>

                        <h2>Minimal PyTorch Pseudocode</h2>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Categorical

class ActorCritic(nn.Module):
    """Shared-backbone actor-critic network."""
    def __init__(self, obs_dim, act_dim, hidden=64):
        super().__init__()
        self.shared = nn.Sequential(
            nn.Linear(obs_dim, hidden),
            nn.Tanh(),
            nn.Linear(hidden, hidden),
            nn.Tanh(),
        )
        self.policy_head = nn.Linear(hidden, act_dim)
        self.value_head = nn.Linear(hidden, 1)

    def forward(self, x):
        features = self.shared(x)
        logits = self.policy_head(features)
        value = self.value_head(features).squeeze(-1)
        return logits, value

    def get_action(self, obs):
        logits, value = self.forward(obs)
        dist = Categorical(logits=logits)
        action = dist.sample()
        return action, dist.log_prob(action), value

    def evaluate(self, obs, actions):
        logits, value = self.forward(obs)
        dist = Categorical(logits=logits)
        return dist.log_prob(actions), dist.entropy(), value


def compute_gae(rewards, values, dones, gamma=0.99, lam=0.95):
    """Generalized Advantage Estimation."""
    advantages = torch.zeros_like(rewards)
    gae = 0.0
    for t in reversed(range(len(rewards))):
        next_value = values[t + 1] if t + 1 &lt; len(values) else 0.0
        delta = rewards[t] + gamma * next_value * (1 - dones[t]) - values[t]
        gae = delta + gamma * lam * (1 - dones[t]) * gae
        advantages[t] = gae
    returns = advantages + values
    return advantages, returns


def ppo_update(
    model, optimizer, rollout,
    clip_eps=0.2, epochs=4, entropy_coef=0.01, value_coef=0.5,
):
    """One PPO update cycle over collected rollout data."""
    obs = rollout["obs"]
    actions = rollout["actions"]
    old_log_probs = rollout["log_probs"]
    advantages = rollout["advantages"]
    returns = rollout["returns"]

    # Normalize advantages
    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

    for epoch in range(epochs):
        # Evaluate current policy on stored transitions
        new_log_probs, entropy, new_values = model.evaluate(obs, actions)

        # Probability ratio
        ratio = torch.exp(new_log_probs - old_log_probs)

        # Clipped surrogate objective
        surr1 = ratio * advantages
        surr2 = torch.clamp(ratio, 1 - clip_eps, 1 + clip_eps) * advantages
        policy_loss = -torch.min(surr1, surr2).mean()

        # Value loss (clipped or simple MSE)
        value_loss = F.mse_loss(new_values, returns)

        # Entropy bonus (encourages exploration)
        entropy_loss = -entropy.mean()

        # Total loss
        loss = policy_loss + value_coef * value_loss + entropy_coef * entropy_loss

        optimizer.zero_grad()
        loss.backward()
        nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
        optimizer.step()

    return {
        "policy_loss": policy_loss.item(),
        "value_loss": value_loss.item(),
        "entropy": entropy.mean().item(),
        "approx_kl": ((ratio - 1) - ratio.log()).mean().item(),
    }
</code></pre>

                        <h2>GAE in Brief</h2>

                        <p>Generalized Advantage Estimation (GAE) computes the advantage by exponentially weighting multi-step TD errors:</p>

<pre><code class="language-python">A_t^GAE = sum_{l=0}^{T-t} (&gamma; &lambda;)^l * &delta;_{t+l}

where &delta;_t = r_t + &gamma; V(s_{t+1}) - V(s_t)
</code></pre>

                        <ul>
                            <li><strong>&lambda; = 0</strong> gives the one-step TD advantage (low variance, high bias).</li>
                            <li><strong>&lambda; = 1</strong> gives the Monte Carlo advantage (high variance, low bias).</li>
                            <li><strong>&lambda; = 0.95</strong> is the standard default, striking a good balance.</li>
                        </ul>

                        <h2>Hyperparameters</h2>

                        <table>
                            <thead>
                                <tr>
                                    <th>Hyperparameter</th>
                                    <th>Typical Value</th>
                                    <th>Notes</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Clip ratio (&epsilon;)</td>
                                    <td>0.1 &ndash; 0.2</td>
                                    <td>0.2 is the most common default</td>
                                </tr>
                                <tr>
                                    <td>GAE &lambda;</td>
                                    <td>0.95</td>
                                    <td>Bias-variance trade-off for advantage</td>
                                </tr>
                                <tr>
                                    <td>Discount &gamma;</td>
                                    <td>0.99</td>
                                    <td>How much the agent values future rewards</td>
                                </tr>
                                <tr>
                                    <td>Learning rate</td>
                                    <td>3e-4</td>
                                    <td>Often annealed linearly to 0</td>
                                </tr>
                                <tr>
                                    <td>PPO epochs (K)</td>
                                    <td>3 &ndash; 10</td>
                                    <td>Reuse each rollout for K gradient updates</td>
                                </tr>
                                <tr>
                                    <td>Mini-batch size</td>
                                    <td>64 &ndash; 512</td>
                                    <td>Depends on rollout size and GPU memory</td>
                                </tr>
                                <tr>
                                    <td>Rollout length (T)</td>
                                    <td>128 &ndash; 2048</td>
                                    <td>Steps collected per iteration</td>
                                </tr>
                                <tr>
                                    <td>Entropy coefficient</td>
                                    <td>0.01</td>
                                    <td>Encourages exploration; too high &rarr; random</td>
                                </tr>
                                <tr>
                                    <td>Value loss coefficient</td>
                                    <td>0.5</td>
                                    <td>Weight of value loss in total loss</td>
                                </tr>
                                <tr>
                                    <td>Max gradient norm</td>
                                    <td>0.5</td>
                                    <td>Gradient clipping for stability</td>
                                </tr>
                            </tbody>
                        </table>

                        <h2>Diagnostics: What to Monitor</h2>

                        <p>Effective PPO training requires watching several metrics:</p>

                        <ul>
                            <li><strong>Approximate KL divergence</strong> &mdash; should stay below ~0.01&ndash;0.05. If it spikes, the policy is changing too fast.</li>
                            <li><strong>Clip fraction</strong> &mdash; the fraction of samples where clipping activated. If near 0, &epsilon; may be too large; if near 1, too small.</li>
                            <li><strong>Explained variance</strong> &mdash; how well the value network predicts actual returns. Should trend toward 1.0.</li>
                            <li><strong>Entropy</strong> &mdash; should decrease gradually as the policy becomes more confident, but not collapse to 0 prematurely.</li>
                            <li><strong>Episode return</strong> &mdash; the primary metric. Should trend upward (with noise).</li>
                        </ul>

                        <h2>PPO vs Alternatives</h2>

                        <table>
                            <thead>
                                <tr>
                                    <th>Method</th>
                                    <th>Pros</th>
                                    <th>Cons</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>PPO</strong></td>
                                    <td>Simple, stable, strong general performance</td>
                                    <td>Sample-inefficient (on-policy)</td>
                                </tr>
                                <tr>
                                    <td><strong>TRPO</strong></td>
                                    <td>Theoretical guarantees on monotonic improvement</td>
                                    <td>Complex (conjugate gradient, line search)</td>
                                </tr>
                                <tr>
                                    <td><strong>SAC</strong></td>
                                    <td>Sample-efficient (off-policy), good for continuous control</td>
                                    <td>Not well-suited for discrete actions / language</td>
                                </tr>
                                <tr>
                                    <td><strong>A2C</strong></td>
                                    <td>Simple baseline</td>
                                    <td>Less stable than PPO, no clipping</td>
                                </tr>
                                <tr>
                                    <td><strong>REINFORCE</strong></td>
                                    <td>Minimal implementation</td>
                                    <td>High variance, impractical for large problems</td>
                                </tr>
                            </tbody>
                        </table>

                        <h2>Practical Tips</h2>

                        <ul>
                            <li><strong>Normalize observations</strong> &mdash; maintain a running mean/std of observations and normalize inputs to the network. This helps both the policy and value networks.</li>
                            <li><strong>Normalize advantages per mini-batch</strong> &mdash; not just per rollout. This prevents stale statistics from distorting gradients.</li>
                            <li><strong>Anneal the learning rate</strong> &mdash; linearly decay from the initial value to 0 over training. This stabilizes the final policy.</li>
                            <li><strong>Use orthogonal initialization</strong> &mdash; for the network weights, with gain sqrt(2) for hidden layers and a small gain (0.01) for the policy head.</li>
                            <li><strong>Separate value and policy networks</strong> &mdash; for complex environments, decoupling the networks can improve value estimation without interfering with policy gradients.</li>
                            <li><strong>Clip the value function</strong> &mdash; similar to policy clipping, clip value updates to prevent large jumps in value estimates.</li>
                        </ul>

                        <h2>PPO in RLHF / NLP Settings</h2>

                        <p>PPO is the RL algorithm behind RLHF (Reinforcement Learning from Human Feedback), used to align language models like ChatGPT. In this setting:</p>

                        <ul>
                            <li>The <strong>policy</strong> is the language model being fine-tuned.</li>
                            <li>The <strong>action space</strong> is the vocabulary (next token prediction).</li>
                            <li>The <strong>reward</strong> comes from a trained reward model that scores the full generated response.</li>
                            <li>A <strong>KL penalty</strong> against the reference (SFT) model is added to prevent the policy from drifting too far.</li>
                        </ul>

<pre><code class="language-python"># RLHF-style reward with KL penalty (pseudocode)
def compute_rlhf_reward(response, reward_model, policy, ref_policy, kl_coef=0.1):
    # Reward from the reward model
    reward_score = reward_model.score(response)

    # KL penalty: penalize divergence from the reference policy
    policy_logprobs = policy.log_prob(response)
    ref_logprobs = ref_policy.log_prob(response)
    kl_penalty = kl_coef * (policy_logprobs - ref_logprobs)

    # Final reward = RM score - KL penalty
    final_reward = reward_score - kl_penalty
    return final_reward
</code></pre>

                        <p>The full RLHF loop requires four models in GPU memory simultaneously: the policy, the reference policy (frozen copy), the reward model, and the value model. This is a primary reason DPO has gained popularity as a simpler alternative.</p>

                        <h2>Failure Modes</h2>

                        <p>PPO is robust but not bulletproof. Common failure modes include:</p>

                        <ul>
                            <li><strong>Reward hacking</strong> &mdash; the policy finds degenerate outputs that score high on the reward model but are not genuinely good. Mitigated by KL penalties and reward model ensembles.</li>
                            <li><strong>Entropy collapse</strong> &mdash; the policy becomes too deterministic too early, getting stuck in a local optimum. Increase the entropy coefficient or use a higher initial temperature.</li>
                            <li><strong>Value function lag</strong> &mdash; if the value network cannot keep up with the policy, advantage estimates become noisy and training destabilizes. Increase value network capacity or value loss coefficient.</li>
                            <li><strong>Stale rollouts</strong> &mdash; if the environment is non-stationary (e.g., self-play), old rollout data may not represent the current dynamics. Use shorter rollouts or more frequent updates.</li>
                            <li><strong>Hyperparameter sensitivity in RLHF</strong> &mdash; the KL coefficient, reward scaling, and generation temperature interact in complex ways. Extensive grid search is often needed.</li>
                        </ul>

                        <h2>Key Takeaways</h2>

                        <ul>
                            <li>PPO constrains policy updates with a <strong>clipped surrogate objective</strong>, preventing destructive large steps.</li>
                            <li>It uses <strong>first-order optimization</strong> (Adam), making it far simpler to implement than TRPO.</li>
                            <li><strong>GAE</strong> provides low-variance advantage estimates that are critical for stable training.</li>
                            <li>PPO is the <strong>standard RL algorithm for RLHF</strong>, powering the alignment of frontier language models.</li>
                            <li>Monitor <strong>KL divergence, clip fraction, entropy, and explained variance</strong> to diagnose training issues.</li>
                            <li>For many alignment tasks, <strong>DPO</strong> offers a simpler alternative that avoids the complexity of the full PPO-based RLHF pipeline.</li>
                        </ul>

                        <p>PPO endures because it occupies a sweet spot: stable enough for production, simple enough to debug, and general enough to work across robotics, games, and language model alignment. Understanding its mechanics is essential for anyone working at the intersection of reinforcement learning and modern AI systems.</p>

                    </div>
                </div>

                <aside class="article-sidebar">
                    <div class="sidebar-author">
                        <img src="images/pm.png" alt="Paolo Mascia" class="sidebar-author-photo">
                        <div class="sidebar-author-name">Paolo Mascia</div>
                        <div class="sidebar-author-role">AI &amp; Cloud Architect</div>
                        <p class="sidebar-author-bio">25+ years designing large-scale distributed systems. Specialized in Generative AI, AI Agents, and cloud-native architectures.</p>
                        <div class="sidebar-author-links">
                            <a href="https://www.linkedin.com/in/paolo-mascia-italy" target="_blank" aria-label="LinkedIn">
                                <svg viewBox="0 0 24 24"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 01-2.063-2.065 2.064 2.064 0 112.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/></svg>
                            </a>
                            <a href="https://github.com/paolomascia" target="_blank" aria-label="GitHub">
                                <svg viewBox="0 0 24 24"><path d="M12 0C5.374 0 0 5.373 0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23A11.509 11.509 0 0112 5.803c1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
                            </a>
                            <a href="https://www.kaggle.com/paolomascia" target="_blank" aria-label="Kaggle">
                                <svg viewBox="0 0 24 24"><path d="M18.825 23.859c-.022.092-.117.141-.281.141h-3.139c-.187 0-.351-.082-.492-.248l-5.178-6.589-1.448 1.374v5.111c0 .235-.117.352-.351.352H5.505c-.236 0-.354-.117-.354-.352V.353c0-.233.118-.353.354-.353h2.431c.234 0 .351.12.351.353v14.343l6.203-6.272c.165-.165.33-.246.495-.246h3.239c.144 0 .236.06.281.18.046.149.034.233-.036.315l-6.555 6.344 6.836 8.507c.095.104.117.208.075.378z"/></svg>
                            </a>
                        </div>
                    </div>
                    <div class="sidebar-box">
                        <h4>More Articles</h4>
                        <a href="post-rlhf.html" class="sidebar-link">
                            <span class="sidebar-link-tag">RLHF</span>
                            <div class="sidebar-link-title">Reinforcement Learning from Human Feedback (RLHF)</div>
                            <span class="sidebar-link-meta">Jun 2025</span>
                        </a>
                        <a href="post-dpo.html" class="sidebar-link">
                            <span class="sidebar-link-tag">DPO</span>
                            <div class="sidebar-link-title">Direct Preference Optimization (DPO)</div>
                            <span class="sidebar-link-meta">Aug 2025</span>
                        </a>
                        <a href="post-rag.html" class="sidebar-link">
                            <span class="sidebar-link-tag">RAG</span>
                            <div class="sidebar-link-title">Retrieval-Augmented Generation (RAG)</div>
                            <span class="sidebar-link-meta">Feb 2025</span>
                        </a>
                        <a href="post-transformers.html" class="sidebar-link">
                            <span class="sidebar-link-tag">Transformers</span>
                            <div class="sidebar-link-title">Transformers: The Architecture That Revolutionized Deep Learning</div>
                            <span class="sidebar-link-meta">Mar 2025</span>
                        </a>
                        <a href="post-random-forest.html" class="sidebar-link">
                            <span class="sidebar-link-tag">Model Evaluation</span>
                            <div class="sidebar-link-title">Random Forest Model Evaluation in Python</div>
                            <span class="sidebar-link-meta">Dec 2023</span>
                        </a>
                    </div>
                </aside>
            </div>
        </div>
    </article>

    <footer class="footer">
        <div class="container">
            <p>&copy; 2025 Paolo Mascia. Built with curiosity and too much coffee.</p>
        </div>
    </footer>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
</body>
</html>