<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Regularization in Linear Regression: Ridge, Lasso, and Elastic Net &mdash; Paolo Mascia</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="article.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
</head>
<body>

    <nav class="navbar scrolled">
        <div class="container">
            <a href="index.html" class="nav-logo">
                <span class="logo-accent">&gt;</span> paolo.mascia
            </a>
            <div class="nav-links">
                <a href="index.html#about">About</a>
                <a href="index.html#projects">Projects</a>
                <a href="ai-tech-insights.html">AI Tech Insights</a>
                <a href="cybersecurity-tech-insights.html">Cybersecurity</a>
                <a href="golang-tech-insights.html">Go</a>
                <a href="index.html#contact">Contact</a>
            </div>
        </div>
    </nav>

    <article class="article">
        <div class="container">
            <div class="article-layout">
                <div class="article-main">
                    <header class="article-header">
                        <a href="ai-tech-insights.html" class="back-link">&larr; Back to Tech Insights</a>
                        <div class="post-meta">
                            <span class="post-date">Jul 2023</span>
                            <span class="post-reading">12 min read</span>
                        </div>
                        <h1>Regularization in Linear Regression: Ridge, Lasso, and Elastic Net</h1>
                        <div class="post-tags">
                            <span>Linear Regression</span>
                            <span>Regularization</span>
                            <span>Python</span>
                        </div>
                    </header>

                    <div class="article-body">
                        <p class="lead">Regularization discourages overly large coefficients by adding a penalty to the cost function. Ridge (L2) smoothly shrinks coefficients, Lasso (L1) forces some to zero for feature selection, and Elastic Net combines both approaches.</p>

                        <h2>Why Regularize?</h2>
                        <p>Ordinary Least Squares (OLS) minimizes the sum of squared residuals with no constraints on coefficient size. When features are correlated, noisy, or numerous, OLS can produce large, unstable coefficients that overfit the training data and generalize poorly.</p>
                        <p>Regularization addresses this by adding a penalty term that discourages large weights:</p>
                        <ul>
                            <li><strong>Reduces overfitting</strong> by constraining model complexity.</li>
                            <li><strong>Improves generalization</strong> on unseen data.</li>
                            <li><strong>Handles multicollinearity</strong> by stabilizing coefficient estimates.</li>
                            <li><strong>Enables feature selection</strong> (Lasso and Elastic Net can zero out irrelevant features).</li>
                        </ul>

                        <h2>Mathematical Formulation</h2>
                        <p>Each regularization method modifies the OLS cost function by adding a penalty controlled by the hyperparameter &alpha;:</p>
                        <table>
                            <thead>
                                <tr>
                                    <th>Method</th>
                                    <th>Cost Function</th>
                                    <th>Penalty Type</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>OLS</td>
                                    <td>&sum;(y<sub>i</sub> &minus; &ycirc;<sub>i</sub>)&sup2;</td>
                                    <td>None</td>
                                </tr>
                                <tr>
                                    <td>Ridge (L2)</td>
                                    <td>&sum;(y<sub>i</sub> &minus; &ycirc;<sub>i</sub>)&sup2; + &alpha; &sum;&beta;<sub>j</sub>&sup2;</td>
                                    <td>Sum of squared coefficients</td>
                                </tr>
                                <tr>
                                    <td>Lasso (L1)</td>
                                    <td>&sum;(y<sub>i</sub> &minus; &ycirc;<sub>i</sub>)&sup2; + &alpha; &sum;|&beta;<sub>j</sub>|</td>
                                    <td>Sum of absolute coefficients</td>
                                </tr>
                                <tr>
                                    <td>Elastic Net</td>
                                    <td>&sum;(y<sub>i</sub> &minus; &ycirc;<sub>i</sub>)&sup2; + &alpha;(&rho; &sum;|&beta;<sub>j</sub>| + (1&minus;&rho;)/2 &sum;&beta;<sub>j</sub>&sup2;)</td>
                                    <td>L1 + L2 combination</td>
                                </tr>
                            </tbody>
                        </table>
                        <p>When &alpha; = 0, all methods reduce to OLS. As &alpha; increases, coefficients shrink toward zero.</p>

                        <h2>Dataset Setup</h2>
                        <p>We use the California Housing dataset, which provides a realistic regression task with correlated features.</p>
                        <pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

data = fetch_california_housing()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = data.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"Training samples: {X_train_scaled.shape[0]}")
print(f"Features: {X_train_scaled.shape[1]}")
print(f"Feature names: {list(X.columns)}")</code></pre>

                        <h2>Ridge Regression (L2)</h2>
                        <p>Ridge adds the squared magnitude of coefficients as a penalty. It never sets coefficients exactly to zero but shrinks them proportionally, making it ideal when all features contribute to the outcome.</p>
                        <pre><code class="language-python">from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, r2_score

alphas = [0.01, 0.1, 1.0, 10.0, 100.0]
results = []

for a in alphas:
    model = Ridge(alpha=a)
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    results.append({"alpha": a, "MSE": round(mse, 4), "R2": round(r2, 4)})

results_df = pd.DataFrame(results)
print(results_df.to_string(index=False))

# Plot coefficients vs alpha
coefs = []
for a in alphas:
    model = Ridge(alpha=a)
    model.fit(X_train_scaled, y_train)
    coefs.append(model.coef_)

plt.figure(figsize=(10, 5))
for i, name in enumerate(X.columns):
    plt.plot(alphas, [c[i] for c in coefs], label=name)
plt.xscale("log")
plt.xlabel("Alpha")
plt.ylabel("Coefficient value")
plt.title("Ridge: Coefficient Shrinkage")
plt.legend(bbox_to_anchor=(1.05, 1), loc="upper left")
plt.tight_layout()
plt.savefig("ridge_coefficients.png", dpi=150)
plt.show()</code></pre>

                        <h2>Lasso Regression (L1)</h2>
                        <p>Lasso uses the absolute value of coefficients as a penalty. This creates a sparse solution &mdash; some coefficients become exactly zero, effectively performing feature selection.</p>
                        <pre><code class="language-python">from sklearn.linear_model import Lasso

alphas = [0.001, 0.01, 0.1, 1.0]
results = []

for a in alphas:
    model = Lasso(alpha=a, max_iter=10000)
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    n_nonzero = np.sum(model.coef_ != 0)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    results.append({"alpha": a, "Non-zero coefs": n_nonzero, "MSE": round(mse, 4), "R2": round(r2, 4)})

results_df = pd.DataFrame(results)
print(results_df.to_string(index=False))

# Plot Lasso coefficient paths
coefs = []
for a in np.logspace(-4, 0, 50):
    model = Lasso(alpha=a, max_iter=10000)
    model.fit(X_train_scaled, y_train)
    coefs.append(model.coef_)

plt.figure(figsize=(10, 5))
for i, name in enumerate(X.columns):
    plt.plot(np.logspace(-4, 0, 50), [c[i] for c in coefs], label=name)
plt.xscale("log")
plt.xlabel("Alpha")
plt.ylabel("Coefficient value")
plt.title("Lasso: Coefficient Paths")
plt.legend(bbox_to_anchor=(1.05, 1), loc="upper left")
plt.tight_layout()
plt.savefig("lasso_coefficients.png", dpi=150)
plt.show()</code></pre>

                        <h2>Elastic Net</h2>
                        <p>Elastic Net combines L1 and L2 penalties via the <code>l1_ratio</code> parameter. When <code>l1_ratio=1</code> it becomes Lasso; when <code>l1_ratio=0</code> it becomes Ridge. Values in between balance sparsity and coefficient stability.</p>
                        <pre><code class="language-python">from sklearn.linear_model import ElasticNet

ratios = [0.1, 0.3, 0.5, 0.7, 0.9]
results = []

for r in ratios:
    model = ElasticNet(alpha=0.01, l1_ratio=r, max_iter=10000)
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    n_nonzero = np.sum(model.coef_ != 0)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    results.append({"l1_ratio": r, "Non-zero coefs": n_nonzero, "MSE": round(mse, 4), "R2": round(r2, 4)})

results_df = pd.DataFrame(results)
print(results_df.to_string(index=False))</code></pre>

                        <h2>Cross-Validation for Optimal Alpha</h2>
                        <p>Scikit-learn provides <code>RidgeCV</code>, <code>LassoCV</code>, and <code>ElasticNetCV</code> which use built-in cross-validation to select the best &alpha; automatically.</p>
                        <pre><code class="language-python">from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV

# RidgeCV
ridge_cv = RidgeCV(alphas=np.logspace(-3, 3, 50), cv=5)
ridge_cv.fit(X_train_scaled, y_train)
print(f"RidgeCV best alpha: {ridge_cv.alpha_:.4f}")
print(f"RidgeCV R2: {ridge_cv.score(X_test_scaled, y_test):.4f}")

# LassoCV
lasso_cv = LassoCV(alphas=np.logspace(-4, 0, 50), cv=5, max_iter=10000)
lasso_cv.fit(X_train_scaled, y_train)
print(f"\nLassoCV best alpha: {lasso_cv.alpha_:.4f}")
print(f"LassoCV R2: {lasso_cv.score(X_test_scaled, y_test):.4f}")

# ElasticNetCV
enet_cv = ElasticNetCV(l1_ratio=[0.1, 0.3, 0.5, 0.7, 0.9], alphas=np.logspace(-4, 0, 50), cv=5, max_iter=10000)
enet_cv.fit(X_train_scaled, y_train)
print(f"\nElasticNetCV best alpha: {enet_cv.alpha_:.4f}")
print(f"ElasticNetCV best l1_ratio: {enet_cv.l1_ratio_:.2f}")
print(f"ElasticNetCV R2: {enet_cv.score(X_test_scaled, y_test):.4f}")</code></pre>

                        <h2>Comparing Regularized Models</h2>
                        <p>Let us train all three regularized models alongside OLS and compare their test-set performance.</p>
                        <pre><code class="language-python">from sklearn.linear_model import LinearRegression

models = {
    "OLS": LinearRegression(),
    "Ridge": Ridge(alpha=ridge_cv.alpha_),
    "Lasso": Lasso(alpha=lasso_cv.alpha_, max_iter=10000),
    "ElasticNet": ElasticNet(alpha=enet_cv.alpha_, l1_ratio=enet_cv.l1_ratio_, max_iter=10000)
}

comparison = []
for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    n_nonzero = np.sum(model.coef_ != 0)
    comparison.append({"Model": name, "MSE": round(mse, 4), "R2": round(r2, 4), "Non-zero coefs": n_nonzero})

comparison_df = pd.DataFrame(comparison)
print(comparison_df.to_string(index=False))</code></pre>

                        <h2>Visualizing Coefficient Shrinkage</h2>
                        <p>A side-by-side comparison of coefficients across methods reveals how each penalty shapes the model.</p>
                        <pre><code class="language-python">fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)

for ax, (name, model) in zip(axes, [("Ridge", Ridge(alpha=1.0)),
                                       ("Lasso", Lasso(alpha=0.01, max_iter=10000)),
                                       ("ElasticNet", ElasticNet(alpha=0.01, l1_ratio=0.5, max_iter=10000))]):
    model.fit(X_train_scaled, y_train)
    ax.barh(X.columns, model.coef_)
    ax.set_title(f"{name} Coefficients")
    ax.axvline(x=0, color="gray", linestyle="--", linewidth=0.8)

plt.tight_layout()
plt.savefig("regularization_comparison.png", dpi=150)
plt.show()</code></pre>

                        <h2>When to Use Each Method</h2>
                        <table>
                            <thead>
                                <tr>
                                    <th>Scenario</th>
                                    <th>Recommended Method</th>
                                    <th>Reason</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>All features likely relevant</td>
                                    <td>Ridge</td>
                                    <td>Shrinks but retains all features</td>
                                </tr>
                                <tr>
                                    <td>Many irrelevant features</td>
                                    <td>Lasso</td>
                                    <td>Drives irrelevant coefficients to zero</td>
                                </tr>
                                <tr>
                                    <td>Correlated features + sparsity needed</td>
                                    <td>Elastic Net</td>
                                    <td>Handles groups of correlated features</td>
                                </tr>
                                <tr>
                                    <td>Few features, clean data</td>
                                    <td>OLS</td>
                                    <td>No regularization needed</td>
                                </tr>
                                <tr>
                                    <td>Unsure which to use</td>
                                    <td>ElasticNetCV</td>
                                    <td>Cross-validation picks the best balance</td>
                                </tr>
                            </tbody>
                        </table>

                        <h2>Key Takeaways</h2>
                        <ul>
                            <li>Regularization prevents overfitting by penalizing large coefficients.</li>
                            <li>Ridge (L2) shrinks coefficients smoothly but never eliminates them.</li>
                            <li>Lasso (L1) produces sparse models by driving some coefficients to exactly zero.</li>
                            <li>Elastic Net combines both penalties and is robust to correlated features.</li>
                            <li>Always standardize features before applying regularization so that penalties are applied fairly.</li>
                            <li>Use cross-validation (<code>RidgeCV</code>, <code>LassoCV</code>, <code>ElasticNetCV</code>) to select the optimal &alpha; automatically.</li>
                        </ul>
                    </div>
                </div>

                <aside class="article-sidebar">
                    <div class="sidebar-author">
                        <img src="images/pm.png" alt="Paolo Mascia" class="sidebar-author-photo">
                        <div class="sidebar-author-name">Paolo Mascia</div>
                        <div class="sidebar-author-role">AI &amp; Cloud Architect</div>
                        <p class="sidebar-author-bio">25+ years designing large-scale distributed systems. Specialized in Generative AI, AI Agents, and cloud-native architectures.</p>
                        <div class="sidebar-author-links">
                            <a href="https://www.linkedin.com/in/paolo-mascia-italy" target="_blank" aria-label="LinkedIn">
                                <svg viewBox="0 0 24 24"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 01-2.063-2.065 2.064 2.064 0 112.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/></svg>
                            </a>
                            <a href="https://github.com/paolomascia" target="_blank" aria-label="GitHub">
                                <svg viewBox="0 0 24 24"><path d="M12 0C5.374 0 0 5.373 0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23A11.509 11.509 0 0112 5.803c1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
                            </a>
                            <a href="https://www.kaggle.com/paolomascia" target="_blank" aria-label="Kaggle">
                                <svg viewBox="0 0 24 24"><path d="M18.825 23.859c-.022.092-.117.141-.281.141h-3.139c-.187 0-.351-.082-.492-.248l-5.178-6.589-1.448 1.374v5.111c0 .235-.117.352-.351.352H5.505c-.236 0-.354-.117-.354-.352V.353c0-.233.118-.353.354-.353h2.431c.234 0 .351.12.351.353v14.343l6.203-6.272c.165-.165.33-.246.495-.246h3.239c.144 0 .236.06.281.18.046.149.034.233-.036.315l-6.555 6.344 6.836 8.507c.095.104.117.208.075.378z"/></svg>
                            </a>
                        </div>
                    </div>
                    <div class="sidebar-box">
                        <h4>More Articles</h4>
                        <a href="post-simple-linear-regression.html" class="sidebar-link">
                            <span class="sidebar-link-tag">Linear Regression</span>
                            <div class="sidebar-link-title">Simple Linear Regression in Python</div>
                            <span class="sidebar-link-meta">Feb 2023</span>
                        </a>
                        <a href="post-multiple-linear-regression.html" class="sidebar-link">
                            <span class="sidebar-link-tag">Linear Regression</span>
                            <div class="sidebar-link-title">Multiple Linear Regression in Python</div>
                            <span class="sidebar-link-meta">May 2023</span>
                        </a>
                        <a href="post-ml-pipelines.html" class="sidebar-link">
                            <span class="sidebar-link-tag">Pipelines</span>
                            <div class="sidebar-link-title">Building Machine Learning Pipelines in Python</div>
                            <span class="sidebar-link-meta">Nov 2025</span>
                        </a>
                        <a href="post-random-forest.html" class="sidebar-link">
                            <span class="sidebar-link-tag">Model Evaluation</span>
                            <div class="sidebar-link-title">Random Forest Model Evaluation</div>
                            <span class="sidebar-link-meta">Dec 2023</span>
                        </a>
                        <a href="post-polynomial-regression.html" class="sidebar-link">
                            <span class="sidebar-link-tag">Polynomial</span>
                            <div class="sidebar-link-title">Polynomial Regression in Python</div>
                            <span class="sidebar-link-meta">Sep 2023</span>
                        </a>
                    </div>
                </aside>
            </div>
        </div>
    </article>

    <footer class="footer">
        <div class="container">
            <p>&copy; 2025 Paolo Mascia. Built with curiosity and too much coffee.</p>
        </div>
    </footer>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
</body>
</html>