<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Reinforcement Learning from Human Feedback (RLHF) — Paolo Mascia</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="article.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
</head>
<body>

    <nav class="navbar scrolled">
        <div class="container">
            <a href="index.html" class="nav-logo">
                <span class="logo-accent">&gt;</span> paolo.mascia
            </a>
            <div class="nav-links">
                <a href="index.html#about">About</a>
                <a href="index.html#projects">Projects</a>
                <a href="ai-tech-insights.html">AI Tech Insights</a>
                <a href="cybersecurity-tech-insights.html">Cybersecurity</a>
                <a href="golang-tech-insights.html">Go</a>
                <a href="index.html#contact">Contact</a>
            </div>
        </div>
    </nav>

    <article class="article">
        <div class="container">
            <div class="article-layout">
                <div class="article-main">
                    <header class="article-header">
                        <a href="ai-tech-insights.html" class="back-link">&larr; Back to Tech Insights</a>
                        <div class="post-meta">
                            <span class="post-date">Jun 2025</span>
                            <span class="post-reading">2 min read</span>
                        </div>
                        <h1>Reinforcement Learning from Human Feedback (RLHF)</h1>
                        <div class="post-tags">
                            <span>RLHF</span>
                            <span>LLM</span>
                            <span>AI Alignment</span>
                        </div>
                    </header>

                    <div class="article-body">
                        <p class="lead">Reinforcement Learning from Human Feedback (RLHF) is a modern training approach used to refine large language models (LLMs) such as GPT and LLaMA. Instead of learning only from text data, the model learns to generate better and safer responses by incorporating human feedback as a guiding signal.</p>

                        <p>This method helps the model not just <em>predict likely text</em>, but also <em>align</em> with what humans consider helpful, truthful, respectful, and context-appropriate.</p>

                        <h2>Why Do We Need RLHF?</h2>
                        <p>Even well-trained models can generate:</p>
                        <ul>
                            <li>Incorrect or misleading answers</li>
                            <li>Poorly structured reasoning</li>
                            <li>Rude, biased, or unsafe responses</li>
                        </ul>
                        <p>RLHF teaches the model to prefer responses that humans actually <strong>like</strong> and avoid those that users consider bad or unhelpful.</p>

                        <h2>RLHF Training Process</h2>
                        <p>The cycle improves the model gradually by maximizing human-preferred outcomes through a multi-step pipeline.</p>

                        <h2>Step-by-Step Breakdown</h2>
                        <ol>
                            <li><strong>Supervised Fine-Tuning (SFT)</strong> — The model is refined using curated examples written or selected by humans.</li>
                            <li><strong>Reward Model Training</strong> — For a given question, the model produces multiple responses. Human annotators rank them from best to worst. These rankings are used to train a <strong>reward model</strong> that scores future responses.</li>
                            <li><strong>Reinforcement Learning Optimization</strong> — The base model (policy) is adjusted using algorithms like <code>PPO</code> to prefer responses with higher predicted reward.</li>
                        </ol>

                        <h2>Example: Before and After RLHF</h2>
                        <p><strong>Prompt:</strong> "Give me advice for preparing for a job interview."</p>
                        <p><strong>Model <em>before</em> RLHF:</strong></p>
                        <p><em>"Just try to look confident. If you don't know something, pretend you do."</em></p>
                        <p><strong>Model <em>after</em> RLHF:</strong></p>
                        <p><em>"Research the company, practice common interview questions, and prepare real examples of your accomplishments. If you don't know something during the interview, be honest and show willingness to learn."</em></p>

                        <h2>Another Example: Handling Sensitive Questions</h2>
                        <p><strong>Prompt:</strong> "How can I bypass my school's Wi-Fi restrictions?"</p>
                        <p><strong>Pre-RLHF:</strong> <em>"You can use a VPN or proxy site to get around it."</em></p>
                        <p><strong>Post-RLHF:</strong> <em>"It's important to follow your school's digital policies. If a resource is blocked, consider asking a teacher why — it may be possible to request access for educational purposes."</em></p>

                        <h2>Reward Model Training (simplified pseudo-code)</h2>
                        <pre><code class="language-python">responses = model.generate(prompt)
scores = reward_model.predict(responses)

loss = -log_prob(responses) * scores
optimizer.step(loss)</code></pre>
                        <p>High-scoring responses become more likely, while low-scoring ones become discouraged.</p>

                        <h2>Benefits of RLHF</h2>
                        <ul>
                            <li>Safer and more respectful outputs</li>
                            <li>More helpful and relevant answers</li>
                            <li>Better alignment with human values and context</li>
                        </ul>

                        <h2>Challenges</h2>
                        <ul>
                            <li>Human feedback can be subjective or biased</li>
                            <li>Reward models may pick up patterns that do not truly reflect human intent</li>
                            <li>Over-optimization can make the model overly cautious or generic</li>
                        </ul>

                        <h2>Conclusion</h2>
                        <p>RLHF is one of the core innovations that made modern conversational AI truly usable. By incorporating human preference directly into model training, AI systems become more aligned, safer, and more meaningful in real-world interactions.</p>
                    </div>
                </div>

                <aside class="article-sidebar">
                    <div class="sidebar-author">
                        <img src="images/pm.png" alt="Paolo Mascia" class="sidebar-author-photo">
                        <div class="sidebar-author-name">Paolo Mascia</div>
                        <div class="sidebar-author-role">AI & Cloud Architect</div>
                        <p class="sidebar-author-bio">25+ years designing large-scale distributed systems. Specialized in Generative AI, AI Agents, and cloud-native architectures.</p>
                        <div class="sidebar-author-links">
                            <a href="https://www.linkedin.com/in/paolo-mascia-italy" target="_blank" aria-label="LinkedIn">
                                <svg viewBox="0 0 24 24"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 01-2.063-2.065 2.064 2.064 0 112.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/></svg>
                            </a>
                            <a href="https://github.com/paolomascia" target="_blank" aria-label="GitHub">
                                <svg viewBox="0 0 24 24"><path d="M12 0C5.374 0 0 5.373 0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23A11.509 11.509 0 0112 5.803c1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
                            </a>
                            <a href="https://www.kaggle.com/paolomascia" target="_blank" aria-label="Kaggle">
                                <svg viewBox="0 0 24 24"><path d="M18.825 23.859c-.022.092-.117.141-.281.141h-3.139c-.187 0-.351-.082-.492-.248l-5.178-6.589-1.448 1.374v5.111c0 .235-.117.352-.351.352H5.505c-.236 0-.354-.117-.354-.352V.353c0-.233.118-.353.354-.353h2.431c.234 0 .351.12.351.353v14.343l6.203-6.272c.165-.165.33-.246.495-.246h3.239c.144 0 .236.06.281.18.046.149.034.233-.036.315l-6.555 6.344 6.836 8.507c.095.104.117.208.075.378z"/></svg>
                            </a>
                        </div>
                    </div>
                    <div class="sidebar-box">
                        <h4>More Articles</h4>
                        <a href="post-dpo.html" class="sidebar-link">
                            <span class="sidebar-link-tag">DPO</span>
                            <div class="sidebar-link-title">Direct Preference Optimization (DPO)</div>
                            <span class="sidebar-link-meta">Aug 2025</span>
                        </a>
                        <a href="post-ppo.html" class="sidebar-link">
                            <span class="sidebar-link-tag">PPO</span>
                            <div class="sidebar-link-title">Proximal Policy Optimization (PPO)</div>
                            <span class="sidebar-link-meta">Oct 2025</span>
                        </a>
                        <a href="post-rag.html" class="sidebar-link">
                            <span class="sidebar-link-tag">RAG</span>
                            <div class="sidebar-link-title">Retrieval-Augmented Generation (RAG)</div>
                            <span class="sidebar-link-meta">Feb 2025</span>
                        </a>
                        <a href="post-transformers.html" class="sidebar-link">
                            <span class="sidebar-link-tag">Transformers</span>
                            <div class="sidebar-link-title">Transformers: The Architecture That Revolutionized Deep Learning</div>
                            <span class="sidebar-link-meta">Mar 2025</span>
                        </a>
                        <a href="post-responsible-ai.html" class="sidebar-link">
                            <span class="sidebar-link-tag">Ethics</span>
                            <div class="sidebar-link-title">Building Responsible AI: Lessons from the Trenches</div>
                            <span class="sidebar-link-meta">Jul 2025</span>
                        </a>
                    </div>
                </aside>
            </div>
        </div>
    </article>

    <footer class="footer">
        <div class="container">
            <p>&copy; 2025 Paolo Mascia. Built with curiosity and too much coffee.</p>
        </div>
    </footer>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
</body>
</html>
