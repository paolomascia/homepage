<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Regression Trees in Python &mdash; Paolo Mascia</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="article.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
</head>
<body>

    <nav class="navbar scrolled">
        <div class="container">
            <a href="index.html" class="nav-logo">
                <span class="logo-accent">&gt;</span> paolo.mascia
            </a>
            <div class="nav-links">
                <a href="index.html#about">About</a>
                <a href="index.html#projects">Projects</a>
                <a href="ai-tech-insights.html">AI Tech Insights</a>
                <a href="cybersecurity-tech-insights.html">Cybersecurity</a>
                <a href="golang-tech-insights.html">Go</a>
                <a href="index.html#contact">Contact</a>
            </div>
        </div>
    </nav>

    <article class="article">
        <div class="container">
            <div class="article-layout">
                <div class="article-main">
                    <header class="article-header">
                        <a href="ai-tech-insights.html" class="back-link">&larr; Back to Tech Insights</a>
                        <div class="post-meta">
                            <span class="post-date">Aug 2023</span>
                            <span class="post-reading">12 min read</span>
                        </div>
                        <h1>Regression Trees in Python</h1>
                        <div class="post-tags">
                            <span>Decision Tree</span>
                            <span>Regression</span>
                            <span>Scikit-Learn</span>
                        </div>
                    </header>

                    <div class="article-body">

                        <p class="lead">A regression tree predicts a continuous target by recursively partitioning the feature space into rectangular regions, then assigning a constant prediction&mdash;typically the mean of training targets&mdash;to each region. It is one of the simplest yet most powerful non-parametric models in machine learning, requiring no assumptions about the functional form of the data.</p>

                        <h2>What Regression Trees Do</h2>

                        <p>At every internal node a regression tree asks a binary question about a single feature: <em>&ldquo;Is feature X<sub>j</sub> &le; threshold?&rdquo;</em> Samples that satisfy the condition go left; the rest go right. The process repeats until a stopping criterion is met&mdash;maximum depth, minimum samples per leaf, or no further reduction in impurity. Each leaf returns a constant value, making the overall prediction a <strong>piecewise constant function</strong> of the inputs.</p>

                        <p>This recursive partitioning strategy is called <strong>CART</strong> (Classification and Regression Trees). Scikit-Learn&rsquo;s <code>DecisionTreeRegressor</code> uses an optimised version of CART that evaluates all possible binary splits on all features at each node and picks the one that minimises the chosen impurity criterion.</p>

                        <h2>Splitting Criteria</h2>

                        <p>Scikit-Learn offers several splitting criteria for regression trees, controlled by the <code>criterion</code> parameter:</p>

                        <ul>
                            <li><strong><code>squared_error</code></strong> (default) &ndash; minimises the mean squared error (MSE) within each resulting partition. Equivalent to variance reduction. This is the most common choice.</li>
                            <li><strong><code>absolute_error</code></strong> &ndash; minimises the mean absolute error (MAE). Uses the median of the targets as the leaf prediction, making it more robust to outliers.</li>
                            <li><strong><code>friedman_mse</code></strong> &ndash; a modified MSE that accounts for potential improvement, used primarily inside gradient-boosting ensembles.</li>
                            <li><strong><code>poisson</code></strong> &ndash; suitable for count data where the target is non-negative and follows a Poisson-like distribution.</li>
                        </ul>

                        <p>For most general-purpose regression tasks, <code>squared_error</code> is the recommended starting point. Switch to <code>absolute_error</code> when your data has heavy-tailed noise or notable outliers.</p>

                        <h2>Key Hyperparameters</h2>

                        <p>The most important hyperparameters for controlling tree complexity are:</p>

                        <ul>
                            <li><strong><code>max_depth</code></strong> &ndash; the maximum depth of the tree. <code>None</code> means the tree grows until all leaves are pure or contain fewer than <code>min_samples_split</code> samples. Setting this to a small value (e.g., 3&ndash;8) is the simplest way to reduce overfitting.</li>
                            <li><strong><code>min_samples_leaf</code></strong> &ndash; the minimum number of samples required to be at a leaf node. Increasing this smooths the model by preventing it from creating leaves with very few observations.</li>
                            <li><strong><code>min_samples_split</code></strong> &ndash; the minimum number of samples required to split an internal node. Defaults to 2.</li>
                            <li><strong><code>max_features</code></strong> &ndash; the number of features to consider when looking for the best split. Setting this below the total number of features introduces randomness and can reduce overfitting.</li>
                            <li><strong><code>ccp_alpha</code></strong> &ndash; complexity parameter for minimal cost-complexity pruning. A non-negative value; higher values produce more aggressive pruning.</li>
                        </ul>

                        <h2>Minimal Example</h2>

                        <p>Let&rsquo;s start with the simplest possible regression tree on a synthetic dataset:</p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor

# Generate noisy sine data
rng = np.random.RandomState(42)
X = np.sort(5 * rng.rand(200, 1), axis=0)
y = np.sin(X).ravel() + 0.3 * rng.randn(200)

# Fit a regression tree
tree = DecisionTreeRegressor(max_depth=4, random_state=42)
tree.fit(X, y)

# Predict on a fine grid
X_grid = np.linspace(0, 5, 500).reshape(-1, 1)
y_pred = tree.predict(X_grid)

# Plot
plt.figure(figsize=(10, 5))
plt.scatter(X, y, s=15, alpha=0.5, label="Training data")
plt.plot(X_grid, y_pred, color="red", linewidth=2, label="Tree prediction")
plt.xlabel("X")
plt.ylabel("y")
plt.title("Regression Tree (max_depth=4)")
plt.legend()
plt.tight_layout()
plt.show()</code></pre>

                        <p>The tree partitions the <em>x</em>-axis into 2<sup>4</sup> = 16 regions (at most) and returns a constant prediction in each. The result is a staircase-like approximation to the underlying sine curve.</p>

                        <h2>Controlling Overfitting with Depth</h2>

                        <p>An unrestricted tree (<code>max_depth=None</code>) will memorise the training data, creating a leaf for nearly every sample. This produces perfect training MSE but terrible generalisation. Limiting depth is the most intuitive regularisation lever:</p>

<pre><code class="language-python">from sklearn.model_selection import cross_val_score

depths = range(1, 20)
cv_scores = []

for d in depths:
    dt = DecisionTreeRegressor(max_depth=d, random_state=42)
    scores = cross_val_score(dt, X, y, cv=5, scoring="neg_mean_squared_error")
    cv_scores.append(-scores.mean())

plt.figure(figsize=(8, 4))
plt.plot(depths, cv_scores, marker="o")
plt.xlabel("max_depth")
plt.ylabel("Mean CV MSE")
plt.title("Cross-Validated MSE vs Tree Depth")
plt.tight_layout()
plt.show()

best_depth = depths[np.argmin(cv_scores)]
print(f"Best depth: {best_depth}")</code></pre>

                        <p>Typically you will see the CV error decrease sharply at first, then flatten or rise as the tree begins overfitting. The optimal depth balances bias and variance.</p>

                        <h2>Cost-Complexity Pruning</h2>

                        <p>An alternative to pre-setting depth is <strong>post-pruning</strong> via the cost-complexity parameter <code>ccp_alpha</code>. The idea is to first grow a full tree, then progressively prune subtrees whose removal increases the total impurity by less than <code>ccp_alpha</code>.</p>

<pre><code class="language-python"># Compute the effective alpha values
tree_full = DecisionTreeRegressor(random_state=42)
tree_full.fit(X, y)

path = tree_full.cost_complexity_pruning_path(X, y)
ccp_alphas = path.ccp_alphas
impurities = path.impurities

# Evaluate each alpha with cross-validation
cv_scores_alpha = []
for alpha in ccp_alphas:
    dt = DecisionTreeRegressor(ccp_alpha=alpha, random_state=42)
    scores = cross_val_score(dt, X, y, cv=5, scoring="neg_mean_squared_error")
    cv_scores_alpha.append(-scores.mean())

plt.figure(figsize=(8, 4))
plt.plot(ccp_alphas, cv_scores_alpha, marker=".", markersize=3)
plt.xlabel("ccp_alpha")
plt.ylabel("Mean CV MSE")
plt.title("Cost-Complexity Pruning Path")
plt.tight_layout()
plt.show()

best_alpha = ccp_alphas[np.argmin(cv_scores_alpha)]
print(f"Best ccp_alpha: {best_alpha:.5f}")

# Refit with the best alpha
tree_pruned = DecisionTreeRegressor(ccp_alpha=best_alpha, random_state=42)
tree_pruned.fit(X, y)
print(f"Pruned tree depth: {tree_pruned.get_depth()}")
print(f"Pruned tree leaves: {tree_pruned.get_n_leaves()}")</code></pre>

                        <p>Cost-complexity pruning is especially useful when you are uncertain about the right depth ahead of time, because it lets the data determine how much of the fully-grown tree to keep.</p>

                        <h2>Interpreting the Tree</h2>

                        <p>One of the main advantages of decision trees is interpretability. Scikit-Learn provides multiple ways to inspect the learned structure.</p>

                        <h3>Text Representation</h3>

<pre><code class="language-python">from sklearn.tree import export_text

tree_small = DecisionTreeRegressor(max_depth=3, random_state=42)
tree_small.fit(X, y)

text_repr = export_text(tree_small, feature_names=["X"])
print(text_repr)</code></pre>

                        <p>This prints a human-readable representation of every split and leaf value.</p>

                        <h3>Feature Importances</h3>

<pre><code class="language-python"># With a multi-feature dataset
from sklearn.datasets import fetch_california_housing

data = fetch_california_housing()
X_cal, y_cal = data.data, data.target
feature_names = data.feature_names

tree_cal = DecisionTreeRegressor(max_depth=6, random_state=42)
tree_cal.fit(X_cal, y_cal)

importances = tree_cal.feature_importances_
sorted_idx = np.argsort(importances)

plt.figure(figsize=(8, 5))
plt.barh(range(len(sorted_idx)), importances[sorted_idx])
plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])
plt.xlabel("Feature Importance (impurity reduction)")
plt.title("Decision Tree Feature Importances")
plt.tight_layout()
plt.show()</code></pre>

                        <p>Feature importance is measured by the total reduction in the splitting criterion attributed to each feature. Note that this metric can be biased toward high-cardinality features; permutation importance is a more robust alternative.</p>

                        <h2>Multi-Feature Regression with Pipelines and Categoricals</h2>

                        <p>Real-world datasets often include both numerical and categorical features. Scikit-Learn pipelines make it straightforward to preprocess and model in a single estimator:</p>

<pre><code class="language-python">import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OrdinalEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Example: Ames Housing (subset)
# Suppose df is a DataFrame with numeric and categorical columns
# For demonstration, we use California Housing with a synthetic categorical
data = fetch_california_housing(as_frame=True)
df = data.frame
df["OceanBucket"] = pd.cut(
    df["Longitude"], bins=4, labels=["West", "MidWest", "MidEast", "East"]
)

target = "MedHouseVal"
features = [c for c in df.columns if c != target]
cat_cols = df[features].select_dtypes(include="category").columns.tolist()
num_cols = [c for c in features if c not in cat_cols]

preprocessor = ColumnTransformer([
    ("cat", OrdinalEncoder(), cat_cols),
], remainder="passthrough")

pipe = Pipeline([
    ("prep", preprocessor),
    ("tree", DecisionTreeRegressor(max_depth=8, random_state=42)),
])

X_train, X_test, y_train, y_test = train_test_split(
    df[features], df[target], test_size=0.2, random_state=42
)

pipe.fit(X_train, y_train)
y_pred = pipe.predict(X_test)

print(f"Test RMSE: {mean_squared_error(y_test, y_pred, squared=False):.4f}")
print(f"Test R²:   {r2_score(y_test, y_pred):.4f}")</code></pre>

                        <blockquote>
                            <strong>Tip:</strong> Decision trees natively handle ordinal encoding without scaling. Unlike linear models, trees are invariant to monotone transformations of features, so you do not need to standardise numeric inputs.
                        </blockquote>

                        <h2>Choosing the Splitting Criterion</h2>

                        <p>When should you switch from the default <code>squared_error</code> to <code>absolute_error</code>?</p>

<pre><code class="language-python"># Compare criteria on data with outliers
rng = np.random.RandomState(0)
X_out = np.sort(rng.rand(300, 1) * 10, axis=0)
y_out = np.sin(X_out).ravel() + 0.2 * rng.randn(300)

# Inject outliers
y_out[::30] += 4 * rng.randn(10)

for criterion in ["squared_error", "absolute_error"]:
    dt = DecisionTreeRegressor(
        criterion=criterion, max_depth=5, random_state=42
    )
    dt.fit(X_out, y_out)
    y_pred = dt.predict(X_grid[:, :1] if X_grid.shape[1] == 1 else X_grid)

    plt.figure(figsize=(9, 4))
    plt.scatter(X_out, y_out, s=12, alpha=0.5, label="Data (with outliers)")
    X_plot = np.linspace(0, 10, 500).reshape(-1, 1)
    plt.plot(X_plot, dt.predict(X_plot), color="red", lw=2, label=f"{criterion}")
    plt.title(f"criterion='{criterion}'")
    plt.legend()
    plt.tight_layout()
    plt.show()</code></pre>

                        <p>With outliers present, <code>absolute_error</code> produces leaf values based on the median rather than the mean, resulting in predictions that are less pulled toward extreme values.</p>

                        <h2>Visualising Predictions vs Ground Truth</h2>

                        <p>A predicted-vs-actual scatter plot is one of the most informative diagnostics for any regression model:</p>

<pre><code class="language-python">tree_final = DecisionTreeRegressor(max_depth=8, random_state=42)
tree_final.fit(X_train.select_dtypes(include="number"),
               y_train)
y_test_pred = tree_final.predict(
    X_test.select_dtypes(include="number")
)

plt.figure(figsize=(6, 6))
plt.scatter(y_test, y_test_pred, s=8, alpha=0.4)
plt.plot([y_test.min(), y_test.max()],
         [y_test.min(), y_test.max()],
         "r--", lw=2, label="Perfect prediction")
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title("Predictions vs Ground Truth")
plt.legend()
plt.tight_layout()
plt.show()</code></pre>

                        <p>Points clustered tightly around the diagonal indicate good fit. Systematic deviations&mdash;for example, underprediction at high values&mdash;reveal where the model struggles.</p>

                        <h2>Hyperparameter Tuning with Cross-Validation</h2>

                        <p>For a thorough search of the hyperparameter space, use <code>GridSearchCV</code> or <code>RandomizedSearchCV</code>:</p>

<pre><code class="language-python">from sklearn.model_selection import GridSearchCV

param_grid = {
    "tree__max_depth": [3, 5, 8, 12, None],
    "tree__min_samples_leaf": [1, 5, 10, 20],
    "tree__min_samples_split": [2, 10, 20],
    "tree__ccp_alpha": [0.0, 0.001, 0.01, 0.05],
}

grid = GridSearchCV(
    pipe,
    param_grid,
    cv=5,
    scoring="neg_root_mean_squared_error",
    n_jobs=-1,
    verbose=1,
)

grid.fit(X_train, y_train)

print(f"Best RMSE (CV): {-grid.best_score_:.4f}")
print(f"Best params:    {grid.best_params_}")

# Evaluate on held-out test set
y_pred_best = grid.predict(X_test)
print(f"Test RMSE:      {mean_squared_error(y_test, y_pred_best, squared=False):.4f}")
print(f"Test R²:        {r2_score(y_test, y_pred_best):.4f}")</code></pre>

                        <p>When the parameter grid is large, <code>RandomizedSearchCV</code> with <code>n_iter</code> samples from the grid can be significantly faster while still finding competitive configurations.</p>

                        <h2>Common Pitfalls</h2>

                        <h3>1. Overfitting</h3>
                        <p>An unrestricted tree will fit every training point perfectly, producing a wildly jagged prediction surface. Always regularise with <code>max_depth</code>, <code>min_samples_leaf</code>, or <code>ccp_alpha</code>. Validate with cross-validation, not training error.</p>

                        <h3>2. Instability (High Variance)</h3>
                        <p>Small changes in the training data can produce a completely different tree structure. This is because a different split at the root cascades through the entire tree. Ensembles like Random Forests and Gradient Boosting mitigate this by averaging many trees.</p>

                        <h3>3. Extrapolation</h3>
                        <p>A regression tree cannot predict values outside the range of the training targets. If the training labels range from 0 to 5, the tree will never predict 6. For datasets where extrapolation matters, consider linear models or hybrid approaches.</p>

                        <h3>4. Axis-Aligned Splits Only</h3>
                        <p>Standard decision trees split on one feature at a time, producing axis-aligned boundaries. They struggle with relationships that are naturally diagonal or circular. Feature engineering (e.g., interaction terms or PCA) can help.</p>

                        <h3>5. Biased Feature Importances</h3>
                        <p>Built-in feature importances favour features with many possible split points (high cardinality). Use <strong>permutation importance</strong> from <code>sklearn.inspection.permutation_importance</code> for a more reliable assessment.</p>

                        <h2>End-to-End Template</h2>

                        <p>Copy this template as a starting point for your own regression tree projects:</p>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.tree import DecisionTreeRegressor, export_text
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OrdinalEncoder
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.inspection import permutation_importance

# ── 1. Load data ───────────────────────────────────────────
data = fetch_california_housing(as_frame=True)
df = data.frame
target = "MedHouseVal"
features = [c for c in df.columns if c != target]

X_train, X_test, y_train, y_test = train_test_split(
    df[features], df[target], test_size=0.2, random_state=42
)

# ── 2. Pipeline ────────────────────────────────────────────
cat_cols = X_train.select_dtypes(include="category").columns.tolist()

preprocessor = ColumnTransformer(
    [("cat", OrdinalEncoder(), cat_cols)],
    remainder="passthrough",
)

pipe = Pipeline([
    ("prep", preprocessor),
    ("tree", DecisionTreeRegressor(random_state=42)),
])

# ── 3. Hyperparameter search ──────────────────────────────
param_grid = {
    "tree__max_depth": [4, 6, 8, 10, None],
    "tree__min_samples_leaf": [1, 5, 10, 20],
    "tree__ccp_alpha": [0.0, 0.005, 0.01],
}

grid = GridSearchCV(
    pipe, param_grid, cv=5,
    scoring="neg_root_mean_squared_error", n_jobs=-1,
)
grid.fit(X_train, y_train)

print("Best CV RMSE:", -grid.best_score_)
print("Best params: ", grid.best_params_)

# ── 4. Evaluate on test set ───────────────────────────────
y_pred = grid.predict(X_test)
print(f"Test RMSE: {mean_squared_error(y_test, y_pred, squared=False):.4f}")
print(f"Test R²:   {r2_score(y_test, y_pred):.4f}")

# ── 5. Diagnostics ────────────────────────────────────────
# Predicted vs actual
plt.figure(figsize=(6, 6))
plt.scatter(y_test, y_pred, s=5, alpha=0.3)
plt.plot([y_test.min(), y_test.max()],
         [y_test.min(), y_test.max()], "r--", lw=2)
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title("Regression Tree: Predicted vs Actual")
plt.tight_layout()
plt.show()

# Permutation importance
best_tree = grid.best_estimator_
result = permutation_importance(
    best_tree, X_test, y_test, n_repeats=10, random_state=42
)
sorted_idx = result.importances_mean.argsort()
plt.figure(figsize=(8, 5))
plt.boxplot(result.importances[sorted_idx].T, vert=False,
            labels=np.array(features)[sorted_idx])
plt.title("Permutation Importance")
plt.tight_layout()
plt.show()</code></pre>

                        <h2>Key Takeaways</h2>

                        <ul>
                            <li>A regression tree partitions the feature space with axis-aligned splits and predicts a constant value per region.</li>
                            <li>Use <code>max_depth</code>, <code>min_samples_leaf</code>, and <code>ccp_alpha</code> to control complexity and prevent overfitting.</li>
                            <li>The <code>squared_error</code> criterion is the default; switch to <code>absolute_error</code> for robustness to outliers.</li>
                            <li>Trees are interpretable: use <code>export_text</code>, feature importances, and tree visualisations.</li>
                            <li>Trees cannot extrapolate beyond the training target range.</li>
                            <li>Individual trees are high-variance; ensembles (Random Forest, Gradient Boosting) dramatically reduce variance.</li>
                            <li>Always validate with cross-validation and inspect residuals.</li>
                        </ul>

                    </div>
                </div>

                <aside class="article-sidebar">
                    <div class="sidebar-author">
                        <img src="images/pm.png" alt="Paolo Mascia" class="sidebar-author-photo">
                        <div class="sidebar-author-name">Paolo Mascia</div>
                        <div class="sidebar-author-role">AI &amp; Cloud Architect</div>
                        <p class="sidebar-author-bio">25+ years designing large-scale distributed systems. Specialized in Generative AI, AI Agents, and cloud-native architectures.</p>
                        <div class="sidebar-author-links">
                            <a href="https://www.linkedin.com/in/paolo-mascia-italy" target="_blank" aria-label="LinkedIn">
                                <svg viewBox="0 0 24 24"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 01-2.063-2.065 2.064 2.064 0 112.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/></svg>
                            </a>
                            <a href="https://github.com/paolomascia" target="_blank" aria-label="GitHub">
                                <svg viewBox="0 0 24 24"><path d="M12 0C5.374 0 0 5.373 0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23A11.509 11.509 0 0112 5.803c1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
                            </a>
                            <a href="https://www.kaggle.com/paolomascia" target="_blank" aria-label="Kaggle">
                                <svg viewBox="0 0 24 24"><path d="M18.825 23.859c-.022.092-.117.141-.281.141h-3.139c-.187 0-.351-.082-.492-.248l-5.178-6.589-1.448 1.374v5.111c0 .235-.117.352-.351.352H5.505c-.236 0-.354-.117-.354-.352V.353c0-.233.118-.353.354-.353h2.431c.234 0 .351.12.351.353v14.343l6.203-6.272c.165-.165.33-.246.495-.246h3.239c.144 0 .236.06.281.18.046.149.034.233-.036.315l-6.555 6.344 6.836 8.507c.095.104.117.208.075.378z"/></svg>
                            </a>
                        </div>
                    </div>
                    <div class="sidebar-box">
                        <h4>More Articles</h4>
                        <ul>
                            <li><a href="post-decision-tree.html">Decision Tree Regression in Python</a><span class="sidebar-post-meta">Decision Tree &middot; Apr 2024</span></li>
                            <li><a href="post-random-forest-regression.html">Random Forest Regression in Python</a><span class="sidebar-post-meta">Random Forest &middot; Mar 2024</span></li>
                            <li><a href="post-random-forest.html">Random Forest Model Evaluation</a><span class="sidebar-post-meta">Model Evaluation &middot; Dec 2023</span></li>
                            <li><a href="post-polynomial-regression.html">Polynomial Regression in Python</a><span class="sidebar-post-meta">Regression &middot; Sep 2023</span></li>
                            <li><a href="post-ml-pipelines.html">Building ML Pipelines in Python</a><span class="sidebar-post-meta">Pipelines &middot; Nov 2025</span></li>
                        </ul>
                    </div>
                </aside>
            </div>
        </div>
    </article>

    <footer class="footer">
        <div class="container">
            <p>&copy; 2025 Paolo Mascia. Built with curiosity and too much coffee.</p>
        </div>
    </footer>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
</body>
</html>