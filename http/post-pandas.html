<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Getting Started with Pandas &mdash; Paolo Mascia</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="article.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
</head>
<body>

    <nav class="navbar scrolled">
        <div class="container">
            <a href="index.html" class="nav-logo">
                <span class="logo-accent">&gt;</span> paolo.mascia
            </a>
            <div class="nav-links">
                <a href="index.html#about">About</a>
                <a href="index.html#projects">Projects</a>
                <a href="ai-tech-insights.html">AI Tech Insights</a>
                <a href="cybersecurity-tech-insights.html">Cybersecurity</a>
                <a href="golang-tech-insights.html">Go</a>
                <a href="index.html#contact">Contact</a>
            </div>
        </div>
    </nav>

    <article class="article">
        <div class="container">
            <div class="article-layout">
                <div class="article-main">
                    <header class="article-header">
                        <a href="ai-tech-insights.html" class="back-link">&larr; Back to Tech Insights</a>
                        <div class="post-meta">
                            <span class="post-date">Jun 2023</span>
                            <span class="post-reading">12 min read</span>
                        </div>
                        <h1>Getting Started with Pandas</h1>
                        <div class="post-tags">
                            <span>Pandas</span>
                            <span>Python</span>
                            <span>Data Analysis</span>
                        </div>
                    </header>

                    <div class="article-body">
                        <p class="lead">Pandas is the cornerstone of data analysis and manipulation in Python. Built on top of NumPy, it provides two powerful data structures &mdash; Series and DataFrame &mdash; that make working with structured data intuitive, fast, and expressive.</p>

                        <p>Whether you&rsquo;re loading a CSV, cleaning messy data, or aggregating millions of rows, pandas gives you the tools to do it in just a few lines of code. This guide covers everything you need to go from zero to productive.</p>

                        <h2>What is Pandas?</h2>
                        <p>Pandas provides two core data structures: the <strong>Series</strong> (a labeled 1D array) and the <strong>DataFrame</strong> (a labeled 2D table). Together, they handle the vast majority of structured data tasks.</p>

                        <pre><code class="language-python">import pandas as pd
import numpy as np

s = pd.Series([10, 20, 30], index=["a", "b", "c"])
print(s)

data = {
    "city": ["New York", "Paris", "Tokyo"],
    "population": [8.4, 2.1, 14.0],
    "continent": ["North America", "Europe", "Asia"]
}
df = pd.DataFrame(data)
print(df)</code></pre>

                        <p>A Series is like a column in a spreadsheet, while a DataFrame is like the entire sheet. Both support labeled indexing, which makes data access far more intuitive than working with raw arrays.</p>

                        <h2>Reading and Writing Data</h2>
                        <p>Pandas supports a wide range of file formats out of the box. You can read from and write to CSV, Excel, JSON, Parquet, SQL databases, and more.</p>

                        <pre><code class="language-python"># CSV
df = pd.read_csv("data.csv")
df.to_csv("output.csv", index=False)

# Excel
df = pd.read_excel("data.xlsx", sheet_name="Sheet1")
df.to_excel("output.xlsx", index=False)

# JSON
df = pd.read_json("data.json")
df.to_json("output.json", orient="records", indent=2)

# Parquet (fast, compressed columnar format)
df = pd.read_parquet("data.parquet")
df.to_parquet("output.parquet", engine="pyarrow")</code></pre>

                        <p>For large datasets, Parquet is strongly recommended over CSV &mdash; it&rsquo;s faster to read, smaller on disk, and preserves data types automatically.</p>

                        <h2>Inspecting and Exploring Data</h2>
                        <p>Before doing any analysis, you should understand the shape and structure of your data.</p>

                        <pre><code class="language-python"># First and last rows
df.head()          # first 5 rows
df.tail(3)         # last 3 rows

# Shape and structure
print(df.shape)    # (rows, columns)
print(df.columns)  # column names
print(df.dtypes)   # data types per column

# Summary information
df.info()          # column types, non-null counts, memory usage

# Statistical summary
df.describe()      # count, mean, std, min, 25%, 50%, 75%, max

# Unique values
print(df["continent"].nunique())        # number of unique values
print(df["continent"].value_counts())   # frequency counts</code></pre>

                        <p><code>df.info()</code> is especially useful for spotting missing values and unexpected data types early in your workflow.</p>

                        <h2>Selecting and Filtering Data</h2>
                        <p>Selecting subsets of data is the most common operation in pandas. There are several ways to do it.</p>

                        <pre><code class="language-python"># Select a single column (returns a Series)
cities = df["city"]

# Select multiple columns (returns a DataFrame)
subset = df[["city", "population"]]

# Filter rows with a boolean condition
large_cities = df[df["population"] > 5]

# Combine conditions with & (and) and | (or)
filtered = df[(df["population"] > 2) & (df["continent"] == "Europe")]

# .loc — label-based selection
df.loc[0, "city"]                # single value
df.loc[0:2, ["city", "population"]]  # slice rows and select columns

# .iloc — integer position-based selection
df.iloc[0, 1]                    # first row, second column
df.iloc[:2, :2]                  # first 2 rows, first 2 columns

# .query() — SQL-like filtering
result = df.query("population > 5 and continent == 'Asia'")</code></pre>

                        <p>Use <code>.loc</code> when you know the labels, <code>.iloc</code> when you know the positions, and <code>.query()</code> when you want readable, SQL-like filters.</p>

                        <h2>Adding, Modifying, and Deleting Columns</h2>
                        <p>DataFrames are mutable &mdash; you can add, modify, or remove columns at any time.</p>

                        <pre><code class="language-python"># Add a new column
df["density"] = df["population"] / 100  # simplified example

# Modify an existing column
df["population"] = df["population"] * 1_000_000

# Conditional column with np.where
df["size"] = np.where(df["population"] > 5_000_000, "large", "small")

# Using .assign() for method chaining
df = df.assign(
    pop_millions=lambda x: x["population"] / 1_000_000,
    is_europe=lambda x: x["continent"] == "Europe"
)

# Delete columns
df = df.drop(columns=["density"])

# Rename columns
df = df.rename(columns={"city": "city_name", "population": "pop"})</code></pre>

                        <h2>Handling Missing Data</h2>
                        <p>Real-world data is almost always incomplete. Pandas uses <code>NaN</code> (Not a Number) to represent missing values and provides powerful tools to handle them.</p>

                        <pre><code class="language-python"># Detect missing values
print(df.isna().sum())          # count of NaN per column
print(df.isna().any())          # True/False per column

# Drop rows with any missing values
df_clean = df.dropna()

# Drop rows where specific columns are NaN
df_clean = df.dropna(subset=["population", "city"])

# Fill missing values with a constant
df["population"] = df["population"].fillna(0)

# Fill with column mean
df["population"] = df["population"].fillna(df["population"].mean())

# Forward fill (carry the previous value forward)
df["value"] = df["value"].ffill()

# Backward fill
df["value"] = df["value"].bfill()

# Interpolate (linear by default)
df["value"] = df["value"].interpolate()</code></pre>

                        <p>Choose your strategy based on context: dropping is appropriate when missing data is rare, imputation (mean, median, forward fill) is better when you need to preserve rows.</p>

                        <h2>Sorting and Reordering</h2>
                        <p>Sorting helps you find top values, rank entries, and organize data for display.</p>

                        <pre><code class="language-python"># Sort by a single column
df_sorted = df.sort_values("population", ascending=False)

# Sort by multiple columns
df_sorted = df.sort_values(["continent", "population"], ascending=[True, False])

# Sort by index
df_sorted = df.sort_index()

# Get the top N rows
top3 = df.nlargest(3, "population")
bottom3 = df.nsmallest(3, "population")

# Reset index after sorting
df_sorted = df_sorted.reset_index(drop=True)

# Ranking
df["pop_rank"] = df["population"].rank(ascending=False, method="dense")</code></pre>

                        <h2>Aggregation and Grouping</h2>
                        <p><code>groupby()</code> is one of the most powerful features in pandas. It splits data into groups, applies a function, and combines the results.</p>

                        <pre><code class="language-python"># Basic groupby
continent_stats = df.groupby("continent")["population"].mean()
print(continent_stats)

# Multiple aggregations
summary = df.groupby("continent").agg(
    total_pop=("population", "sum"),
    avg_pop=("population", "mean"),
    num_cities=("city", "count")
)
print(summary)

# Aggregation with different functions per column
agg = df.groupby("continent").agg({
    "population": ["sum", "mean"],
    "city": "count"
})

# Using transform to add group-level stats as new columns
df["continent_avg"] = df.groupby("continent")["population"].transform("mean")

# Percentage of group total
df["pct_of_continent"] = (
    df["population"] / df.groupby("continent")["population"].transform("sum") * 100
)</code></pre>

                        <p>The <strong>split-apply-combine</strong> pattern behind <code>groupby()</code> handles the vast majority of aggregation tasks you&rsquo;ll encounter.</p>

                        <h2>Merging and Joining Datasets</h2>
                        <p>Combining multiple DataFrames is essential when your data lives in separate tables or files.</p>

                        <pre><code class="language-python"># Sample DataFrames
orders = pd.DataFrame({
    "order_id": [1, 2, 3, 4],
    "customer_id": [101, 102, 101, 103],
    "amount": [50, 75, 30, 100]
})

customers = pd.DataFrame({
    "customer_id": [101, 102, 104],
    "name": ["Alice", "Bob", "Diana"]
})

# Inner join (only matching rows)
merged = pd.merge(orders, customers, on="customer_id", how="inner")

# Left join (all rows from left, matched from right)
merged = pd.merge(orders, customers, on="customer_id", how="left")

# Right join
merged = pd.merge(orders, customers, on="customer_id", how="right")

# Outer join (all rows from both)
merged = pd.merge(orders, customers, on="customer_id", how="outer")

# Concatenate DataFrames vertically
df_all = pd.concat([df1, df2, df3], ignore_index=True)

# Concatenate horizontally
df_wide = pd.concat([df_a, df_b], axis=1)</code></pre>

                        <p>The <code>how</code> parameter works exactly like SQL joins. Use <code>"left"</code> when you want to preserve all rows from your primary table and enrich them with data from a lookup table.</p>

                        <h2>Pivoting and Reshaping Data</h2>
                        <p>Sometimes you need to reshape data between &ldquo;long&rdquo; and &ldquo;wide&rdquo; formats.</p>

                        <pre><code class="language-python"># Sample long-format data
long_df = pd.DataFrame({
    "city": ["Paris", "Paris", "London", "London"],
    "metric": ["sales", "profit", "sales", "profit"],
    "value": [120, 30, 90, 20]
})

# Pivot: long to wide
wide_df = long_df.pivot(index="city", columns="metric", values="value")
print(wide_df)

# Melt: wide to long
melted = wide_df.reset_index().melt(
    id_vars="city",
    value_vars=["sales", "profit"],
    var_name="metric",
    value_name="value"
)
print(melted)

# Pivot table with aggregation
sales_df = pd.DataFrame({
    "city": ["Paris", "Paris", "London", "London"],
    "quarter": ["Q1", "Q2", "Q1", "Q2"],
    "sales": [100, 150, 90, 120]
})
pivot = pd.pivot_table(sales_df, values="sales", index="city",
                        columns="quarter", aggfunc="sum")
print(pivot)</code></pre>

                        <h2>Working with Strings</h2>
                        <p>Pandas provides vectorized string operations through the <code>.str</code> accessor, so you never need to loop over rows for text manipulation.</p>

                        <pre><code class="language-python"># Common string operations
df["city_upper"] = df["city"].str.upper()
df["city_lower"] = df["city"].str.lower()
df["city_len"] = df["city"].str.len()

# Check for substrings
df["has_new"] = df["city"].str.contains("New", na=False)

# Extract patterns with regex
df["first_word"] = df["city"].str.extract(r"(\w+)")

# Replace
df["city_clean"] = df["city"].str.replace(" ", "_")

# Split
df[["first", "last"]] = df["city"].str.split(" ", n=1, expand=True)

# Strip whitespace
df["city"] = df["city"].str.strip()

# String slicing
df["city_code"] = df["city"].str[:3].str.upper()</code></pre>

                        <p>All <code>.str</code> methods handle <code>NaN</code> values gracefully by default, returning <code>NaN</code> for missing entries without raising errors.</p>

                        <h2>Basic Visualization with Pandas</h2>
                        <p>Pandas integrates directly with matplotlib, letting you create quick exploratory plots from any DataFrame or Series.</p>

                        <pre><code class="language-python">import matplotlib.pyplot as plt

# Line plot
df["population"].plot(kind="line", title="Population")
plt.show()

# Bar chart
df.groupby("continent")["population"].sum().plot(kind="bar", title="Population by Continent")
plt.ylabel("Population")
plt.tight_layout()
plt.show()

# Histogram
df["population"].plot(kind="hist", bins=10, title="Population Distribution")
plt.show()

# Scatter plot
df.plot(kind="scatter", x="population", y="density", title="Pop vs Density")
plt.show()

# Box plot
df.boxplot(column="population", by="continent")
plt.suptitle("Population by Continent")
plt.show()

# Multiple subplots
fig, axes = plt.subplots(1, 2, figsize=(12, 5))
df["population"].plot(kind="bar", ax=axes[0], title="Population")
df["density"].plot(kind="bar", ax=axes[1], title="Density")
plt.tight_layout()
plt.show()</code></pre>

                        <p>Pandas plots are great for quick exploration. For polished visualizations, consider seaborn or plotly, which build on top of the same foundations.</p>

                        <h2>Key Takeaways</h2>
                        <ul>
                            <li><strong>Series</strong> and <strong>DataFrame</strong> are the two core structures &mdash; understand them and everything else follows</li>
                            <li><strong>read_csv()</strong> / <strong>to_csv()</strong> handle the most common data format, but prefer <strong>Parquet</strong> for performance</li>
                            <li>Always <strong>inspect your data</strong> first with <code>head()</code>, <code>info()</code>, and <code>describe()</code></li>
                            <li>Use <strong>.loc</strong> for label-based and <strong>.iloc</strong> for position-based selection</li>
                            <li><strong>Missing data</strong> handling is essential &mdash; choose between dropping, filling, or interpolating based on context</li>
                            <li><strong>groupby()</strong> with named aggregation is the standard pattern for summarizing data</li>
                            <li><strong>merge()</strong> works like SQL joins &mdash; master inner, left, right, and outer joins</li>
                            <li><strong>pivot()</strong> and <strong>melt()</strong> let you reshape between long and wide formats</li>
                            <li><strong>Vectorized .str</strong> methods replace slow row-by-row text processing</li>
                            <li>Built-in <strong>.plot()</strong> gives you instant visualizations for exploratory analysis</li>
                        </ul>
                    </div>
                </div>

                <aside class="article-sidebar">
                    <div class="sidebar-author">
                        <img src="images/pm.png" alt="Paolo Mascia" class="sidebar-author-photo">
                        <div class="sidebar-author-name">Paolo Mascia</div>
                        <div class="sidebar-author-role">AI & Cloud Architect</div>
                        <p class="sidebar-author-bio">25+ years designing large-scale distributed systems. Specialized in Generative AI, AI Agents, and cloud-native architectures.</p>
                        <div class="sidebar-author-links">
                            <a href="https://www.linkedin.com/in/paolo-mascia-italy" target="_blank" aria-label="LinkedIn">
                                <svg viewBox="0 0 24 24"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 01-2.063-2.065 2.064 2.064 0 112.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/></svg>
                            </a>
                            <a href="https://github.com/paolomascia" target="_blank" aria-label="GitHub">
                                <svg viewBox="0 0 24 24"><path d="M12 0C5.374 0 0 5.373 0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23A11.509 11.509 0 0112 5.803c1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
                            </a>
                            <a href="https://www.kaggle.com/paolomascia" target="_blank" aria-label="Kaggle">
                                <svg viewBox="0 0 24 24"><path d="M18.825 23.859c-.022.092-.117.141-.281.141h-3.139c-.187 0-.351-.082-.492-.248l-5.178-6.589-1.448 1.374v5.111c0 .235-.117.352-.351.352H5.505c-.236 0-.354-.117-.354-.352V.353c0-.233.118-.353.354-.353h2.431c.234 0 .351.12.351.353v14.343l6.203-6.272c.165-.165.33-.246.495-.246h3.239c.144 0 .236.06.281.18.046.149.034.233-.036.315l-6.555 6.344 6.836 8.507c.095.104.117.208.075.378z"/></svg>
                            </a>
                        </div>
                    </div>
                    <div class="sidebar-box">
                        <h4>More Articles</h4>
                        <a href="post-advanced-pandas.html" class="sidebar-link">
                            <span class="sidebar-link-tag">Pandas</span>
                            <div class="sidebar-link-title">Advanced Pandas: Method Chaining, MultiIndex, Time Series, and Performance</div>
                            <span class="sidebar-link-meta">Nov 2023</span>
                        </a>
                        <a href="post-numpy.html" class="sidebar-link">
                            <span class="sidebar-link-tag">NumPy</span>
                            <div class="sidebar-link-title">Mastering NumPy: The Foundation of Scientific Computing in Python</div>
                            <span class="sidebar-link-meta">Mar 2023</span>
                        </a>
                        <a href="post-ml-pipelines.html" class="sidebar-link">
                            <span class="sidebar-link-tag">Pipelines</span>
                            <div class="sidebar-link-title">Building Machine Learning Pipelines in Python</div>
                            <span class="sidebar-link-meta">Nov 2025</span>
                        </a>
                        <a href="post-simple-linear-regression.html" class="sidebar-link">
                            <span class="sidebar-link-tag">Linear Regression</span>
                            <div class="sidebar-link-title">Simple Linear Regression in Python</div>
                            <span class="sidebar-link-meta">Feb 2023</span>
                        </a>
                        <a href="post-multiple-linear-regression.html" class="sidebar-link">
                            <span class="sidebar-link-tag">Linear Regression</span>
                            <div class="sidebar-link-title">Multiple Linear Regression in Python</div>
                            <span class="sidebar-link-meta">May 2023</span>
                        </a>
                    </div>
                </aside>
            </div>
        </div>
    </article>

    <footer class="footer">
        <div class="container">
            <p>&copy; 2025 Paolo Mascia. Built with curiosity and too much coffee.</p>
        </div>
    </footer>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
</body>
</html>