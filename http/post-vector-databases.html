<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Vector Databases: Storage and Retrieval for Embedding-Based AI Systems â€” Paolo Mascia</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="article.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
</head>
<body>

    <nav class="navbar scrolled">
        <div class="container">
            <a href="index.html" class="nav-logo">
                <span class="logo-accent">&gt;</span> paolo.mascia
            </a>
            <div class="nav-links">
                <a href="index.html#about">About</a>
                <a href="index.html#projects">Projects</a>
                <a href="ai-tech-insights.html">AI Tech Insights</a>
                <a href="cybersecurity-tech-insights.html">Cybersecurity</a>
                <a href="golang-tech-insights.html">Go</a>
                <a href="index.html#contact">Contact</a>
            </div>
        </div>
    </nav>

    <article class="article">
        <div class="container">
            <div class="article-layout">
                <div class="article-main">
                    <header class="article-header">
                        <a href="ai-tech-insights.html" class="back-link">&larr; Back to Tech Insights</a>
                        <div class="post-meta">
                            <span class="post-date">May 2025</span>
                            <span class="post-reading">14 min read</span>
                        </div>
                        <h1>Vector Databases: Storage and Retrieval for Embedding-Based AI Systems</h1>
                        <div class="post-tags">
                            <span>Vector Databases</span>
                            <span>Embeddings</span>
                            <span>FAISS</span>
                        </div>
                    </header>

                    <div class="article-body">

                        <p>Vector databases are specialized storage systems designed to index, store, and retrieve high-dimensional vectors efficiently. They are a foundational component in modern AI systems &mdash; from semantic search and recommendation engines to Retrieval-Augmented Generation (RAG) pipelines.</p>

                        <p>Unlike traditional relational databases that operate on structured rows and columns, vector databases are optimized for <strong>nearest-neighbor search</strong> in high-dimensional spaces. This article explores why they matter, how they work under the hood, and how to use FAISS &mdash; one of the most widely adopted vector search libraries &mdash; in practice.</p>

                        <h2>Why Vector Databases Matter</h2>

                        <p>Modern AI models &mdash; particularly large language models (LLMs) and vision transformers &mdash; represent data as dense numerical vectors called <strong>embeddings</strong>. A sentence, an image, or a product description is mapped to a point in a high-dimensional space (typically 384 to 4096 dimensions). The core insight is simple: <em>semantically similar items end up close together in that space</em>.</p>

                        <p>This gives rise to a fundamental operation: <strong>given a query vector, find the most similar vectors in a collection</strong>. Traditional databases are not designed for this. B-tree indices, hash maps, and SQL query planners have no concept of &ldquo;nearby in 768-dimensional space.&rdquo; Vector databases fill that gap.</p>

                        <p>Key use cases include:</p>
                        <ul>
                            <li><strong>Semantic search</strong> &mdash; finding documents by meaning, not keywords</li>
                            <li><strong>Recommendation engines</strong> &mdash; retrieving items similar to a user&rsquo;s preferences</li>
                            <li><strong>RAG pipelines</strong> &mdash; grounding LLM responses in retrieved facts</li>
                            <li><strong>Deduplication</strong> &mdash; detecting near-duplicate content at scale</li>
                            <li><strong>Anomaly detection</strong> &mdash; finding points far from any cluster center</li>
                        </ul>

                        <h2>Embedding Representations</h2>

                        <p>Before you can search vectors, you need to produce them. An <strong>embedding model</strong> maps raw input (text, images, audio) to a fixed-size numerical vector. For text, the most common choice today is a Sentence Transformer:</p>

<pre><code class="language-python">from sentence_transformers import SentenceTransformer
import numpy as np

model = SentenceTransformer("all-MiniLM-L6-v2")

documents = [
    "Vector databases store high-dimensional embeddings.",
    "FAISS is a library for efficient similarity search.",
    "Retrieval-Augmented Generation improves LLM accuracy.",
    "Cosine similarity measures the angle between two vectors.",
    "Product quantization compresses vectors for large-scale search.",
]

embeddings = model.encode(documents, normalize_embeddings=True)
print(f"Shape: {embeddings.shape}")  # (5, 384)
print(f"Dtype: {embeddings.dtype}")  # float32
</code></pre>

                        <p>Each document is now a 384-dimensional unit vector. The <code>normalize_embeddings=True</code> flag ensures every vector has unit norm, which is important when using inner-product-based similarity.</p>

                        <h2>Similarity Metrics</h2>

                        <p>Vector databases retrieve results by computing a <strong>similarity</strong> (or distance) between the query vector and every stored vector. The three most common metrics are:</p>

                        <table>
                            <thead>
                                <tr>
                                    <th>Metric</th>
                                    <th>Formula</th>
                                    <th>Range</th>
                                    <th>Notes</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>Cosine Similarity</strong></td>
                                    <td>cos(&theta;) = (A &middot; B) / (||A|| ||B||)</td>
                                    <td>[-1, 1]</td>
                                    <td>Direction-based; ignores magnitude</td>
                                </tr>
                                <tr>
                                    <td><strong>L2 (Euclidean) Distance</strong></td>
                                    <td>||A - B||&sup2;</td>
                                    <td>[0, &infin;)</td>
                                    <td>Sensitive to magnitude; lower is closer</td>
                                </tr>
                                <tr>
                                    <td><strong>Inner Product (Dot Product)</strong></td>
                                    <td>A &middot; B</td>
                                    <td>(-&infin;, &infin;)</td>
                                    <td>Equals cosine similarity when vectors are normalized</td>
                                </tr>
                            </tbody>
                        </table>

                        <p><strong>Practical tip:</strong> If you normalize all your embeddings to unit length, inner product and cosine similarity become equivalent. FAISS&rsquo;s <code>IndexFlatIP</code> then gives you exact cosine similarity search &mdash; and it is faster than computing cosine explicitly.</p>

                        <h2>FAISS: Fast, Scalable Vector Search</h2>

                        <p><strong>FAISS</strong> (Facebook AI Similarity Search) is an open-source library developed by Meta AI Research. It provides highly optimized CPU and GPU implementations of nearest-neighbor search algorithms. FAISS is not a database in itself &mdash; it is a search index library &mdash; but it is the engine inside many vector database products.</p>

                        <h3>Exact Search with IndexFlatIP</h3>

                        <p>The simplest FAISS index performs <strong>brute-force</strong> inner-product search. It compares the query against every stored vector, guaranteeing perfect recall:</p>

<pre><code class="language-python">import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("all-MiniLM-L6-v2")

documents = [
    "Vector databases store high-dimensional embeddings.",
    "FAISS is a library for efficient similarity search.",
    "Retrieval-Augmented Generation improves LLM accuracy.",
    "Cosine similarity measures the angle between two vectors.",
    "Product quantization compresses vectors for large-scale search.",
]

embeddings = model.encode(documents, normalize_embeddings=True)
dimension = embeddings.shape[1]  # 384

# Create a flat inner-product index
index = faiss.IndexFlatIP(dimension)
index.add(embeddings)

print(f"Total vectors in index: {index.ntotal}")  # 5

# Query
query = "How do I measure similarity between embeddings?"
query_vec = model.encode([query], normalize_embeddings=True)

scores, indices = index.search(query_vec, k=3)

print("\nTop 3 results:")
for rank, (idx, score) in enumerate(zip(indices[0], scores[0])):
    print(f"  {rank+1}. [{score:.4f}] {documents[idx]}")
</code></pre>

                        <p>Output:</p>

<pre><code class="language-python">Top 3 results:
  1. [0.7523] Cosine similarity measures the angle between two vectors.
  2. [0.6241] Vector databases store high-dimensional embeddings.
  3. [0.5839] FAISS is a library for efficient similarity search.
</code></pre>

                        <h3>Adding IDs with IndexIDMap2</h3>

                        <p>By default, FAISS assigns sequential integer IDs starting from 0. If you need custom IDs (e.g., database primary keys), wrap the base index with <code>IndexIDMap2</code>:</p>

<pre><code class="language-python">import faiss
import numpy as np

dimension = 384
base_index = faiss.IndexFlatIP(dimension)
index = faiss.IndexIDMap2(base_index)

# Custom IDs (e.g., document IDs from your database)
custom_ids = np.array([1001, 1002, 1003, 1004, 1005], dtype=np.int64)

index.add_with_ids(embeddings, custom_ids)

scores, ids = index.search(query_vec, k=3)
print("Returned IDs:", ids[0])  # e.g., [1004, 1001, 1002]
</code></pre>

                        <p><code>IndexIDMap2</code> also supports <code>remove_ids()</code>, making it suitable for indices that need to evolve over time.</p>

                        <h2>Scaling with IVF and Product Quantization</h2>

                        <p>Brute-force search is exact but <strong>O(n &middot; d)</strong> per query, where <em>n</em> is the number of vectors and <em>d</em> is the dimensionality. For millions or billions of vectors, this is too slow. FAISS offers two complementary strategies:</p>

                        <h3>Inverted File Index (IVF)</h3>

                        <p>IVF partitions the vector space into <strong>nlist</strong> clusters using k-means. At query time, only the <strong>nprobe</strong> nearest clusters are searched, reducing the number of comparisons dramatically:</p>

<pre><code class="language-python">import faiss
import numpy as np

dimension = 384
nlist = 100      # number of Voronoi cells (clusters)
nprobe = 10      # number of cells to search at query time

# IVF requires a quantizer (the coarse-level index)
quantizer = faiss.IndexFlatIP(dimension)
index = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_INNER_PRODUCT)

# IVF must be trained on representative data before adding vectors
training_data = np.random.randn(10000, dimension).astype("float32")
faiss.normalize_L2(training_data)

index.train(training_data)
index.add(training_data)

# Set the number of probes for search
index.nprobe = nprobe

scores, indices = index.search(query_vec, k=5)
</code></pre>

                        <p>The trade-off: higher <code>nprobe</code> gives better recall but slower search. A common rule of thumb is <code>nlist = sqrt(n)</code> and <code>nprobe = nlist / 10</code>.</p>

                        <h3>Product Quantization (PQ)</h3>

                        <p>PQ compresses each vector from <em>d &times; 4 bytes</em> (float32) down to <em>m bytes</em>, where <em>m</em> is the number of sub-quantizers. This reduces memory by 10&ndash;100&times; with moderate recall loss:</p>

<pre><code class="language-python">import faiss

dimension = 384
nlist = 100
m = 48           # number of sub-quantizers (must divide dimension)
nbits = 8        # bits per sub-quantizer (256 centroids each)

quantizer = faiss.IndexFlatIP(dimension)
index = faiss.IndexIVFPQ(
    quantizer, dimension, nlist, m, nbits,
    faiss.METRIC_INNER_PRODUCT
)

# Train on representative data
index.train(training_data)
index.add(training_data)

index.nprobe = 10
scores, indices = index.search(query_vec, k=5)
</code></pre>

                        <p>With PQ, each 384-dimensional vector is compressed from 1,536 bytes to just 48 bytes &mdash; a 32&times; reduction. For a 100-million-vector collection, this means ~4.5 GB instead of ~143 GB.</p>

                        <h2>GPU Acceleration</h2>

                        <p>FAISS supports transparent GPU acceleration. Moving an index to GPU can speed up both training and search by 5&ndash;50&times;:</p>

<pre><code class="language-python">import faiss

# Create a CPU index
cpu_index = faiss.IndexFlatIP(384)
cpu_index.add(embeddings)

# Move to GPU (device 0)
gpu_resource = faiss.StandardGpuResources()
gpu_index = faiss.index_cpu_to_gpu(gpu_resource, 0, cpu_index)

# Search on GPU
scores, indices = gpu_index.search(query_vec, k=5)
</code></pre>

                        <p>For multi-GPU setups, use <code>faiss.index_cpu_to_all_gpus(cpu_index)</code> to shard the index across all available GPUs automatically.</p>

                        <h2>Hybrid Retrieval: BM25 + FAISS</h2>

                        <p>Semantic (vector) search excels at understanding meaning, but it can miss exact keyword matches. Lexical search (BM25) is great at precise term matching but blind to synonyms and paraphrases. <strong>Hybrid retrieval</strong> combines both:</p>

<pre><code class="language-python">from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

documents = [
    "FAISS supports GPU-accelerated similarity search.",
    "BM25 is a probabilistic ranking function for text retrieval.",
    "Vector databases index high-dimensional embeddings.",
    "Product quantization reduces memory usage in FAISS.",
    "Hybrid search combines lexical and semantic retrieval.",
]

# --- BM25 (lexical) ---
tokenized_docs = [doc.lower().split() for doc in documents]
bm25 = BM25Okapi(tokenized_docs)

# --- FAISS (semantic) ---
model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = model.encode(documents, normalize_embeddings=True)

index = faiss.IndexFlatIP(embeddings.shape[1])
index.add(embeddings)

def hybrid_search(query, k=3, alpha=0.5):
    """
    Combine BM25 and FAISS scores.
    alpha = 1.0 &rarr; pure semantic
    alpha = 0.0 &rarr; pure lexical
    """
    # BM25 scores
    tokenized_query = query.lower().split()
    bm25_scores = bm25.get_scores(tokenized_query)
    bm25_scores = bm25_scores / (bm25_scores.max() + 1e-8)  # normalize

    # FAISS scores
    query_vec = model.encode([query], normalize_embeddings=True)
    faiss_scores, faiss_ids = index.search(query_vec, k=len(documents))
    semantic_scores = np.zeros(len(documents))
    for idx, score in zip(faiss_ids[0], faiss_scores[0]):
        semantic_scores[idx] = score

    # Combine
    combined = alpha * semantic_scores + (1 - alpha) * bm25_scores
    top_k = np.argsort(combined)[::-1][:k]

    results = []
    for idx in top_k:
        results.append({
            "document": documents[idx],
            "combined_score": combined[idx],
            "semantic_score": semantic_scores[idx],
            "bm25_score": bm25_scores[idx],
        })
    return results

results = hybrid_search("How does FAISS use GPU?", k=3, alpha=0.6)
for r in results:
    print(f"[{r['combined_score']:.4f}] {r['document']}")
</code></pre>

                        <p>The <code>alpha</code> parameter controls the blend. In practice, values between 0.5 and 0.7 work well for most RAG applications.</p>

                        <h2>Re-ranking with Cross-Encoders</h2>

                        <p>Bi-encoder retrieval (FAISS) is fast but approximate &mdash; it encodes query and document independently. A <strong>cross-encoder</strong> processes the query-document pair jointly, producing a more accurate relevance score at the cost of higher latency:</p>

<pre><code class="language-python">from sentence_transformers import CrossEncoder

# Load a cross-encoder model
reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

query = "How does FAISS use GPU acceleration?"
candidates = [
    "FAISS supports GPU-accelerated similarity search.",
    "BM25 is a probabilistic ranking function for text retrieval.",
    "Vector databases index high-dimensional embeddings.",
]

# Score each (query, candidate) pair
pairs = [(query, doc) for doc in candidates]
scores = reranker.predict(pairs)

# Sort by score descending
ranked = sorted(
    zip(candidates, scores),
    key=lambda x: x[1],
    reverse=True,
)

print("Re-ranked results:")
for doc, score in ranked:
    print(f"  [{score:.4f}] {doc}")
</code></pre>

                        <p>The typical pattern in production is a <strong>two-stage pipeline</strong>:</p>
                        <ol>
                            <li><strong>Stage 1 &mdash; Retrieval:</strong> Use FAISS (or hybrid BM25 + FAISS) to fetch the top 50&ndash;100 candidates quickly.</li>
                            <li><strong>Stage 2 &mdash; Re-ranking:</strong> Use a cross-encoder to re-score and re-order the top candidates, returning the final top-k.</li>
                        </ol>

                        <p>This gives you both <strong>speed</strong> (from the bi-encoder) and <strong>accuracy</strong> (from the cross-encoder), which is the standard architecture for production-grade semantic search and RAG systems.</p>

                        <h2>Choosing the Right Index</h2>

                        <p>FAISS offers many index types. Here is a practical decision guide:</p>

                        <table>
                            <thead>
                                <tr>
                                    <th>Collection Size</th>
                                    <th>Recommended Index</th>
                                    <th>Recall</th>
                                    <th>Speed</th>
                                    <th>Memory</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>&lt; 100K vectors</td>
                                    <td>IndexFlatIP</td>
                                    <td>100%</td>
                                    <td>Good</td>
                                    <td>High</td>
                                </tr>
                                <tr>
                                    <td>100K &ndash; 10M</td>
                                    <td>IndexIVFFlat</td>
                                    <td>~95%+</td>
                                    <td>Fast</td>
                                    <td>High</td>
                                </tr>
                                <tr>
                                    <td>10M &ndash; 1B</td>
                                    <td>IndexIVFPQ</td>
                                    <td>~85&ndash;95%</td>
                                    <td>Very fast</td>
                                    <td>Low</td>
                                </tr>
                                <tr>
                                    <td>&gt; 1B vectors</td>
                                    <td>IndexIVFPQ + GPU sharding</td>
                                    <td>~80&ndash;90%</td>
                                    <td>Very fast</td>
                                    <td>Low</td>
                                </tr>
                            </tbody>
                        </table>

                        <h2>Key Takeaways</h2>

                        <ul>
                            <li>Vector databases are purpose-built for <strong>nearest-neighbor search</strong> in high-dimensional embedding spaces.</li>
                            <li>Normalize your embeddings and use <strong>inner product</strong> (IndexFlatIP) for cosine similarity search.</li>
                            <li>Use <strong>IVF</strong> to partition the search space and <strong>PQ</strong> to compress vectors when scaling beyond millions of records.</li>
                            <li>Combine <strong>BM25 + FAISS</strong> for hybrid retrieval that captures both keyword and semantic relevance.</li>
                            <li>Add a <strong>cross-encoder re-ranker</strong> as a second stage for higher precision in the final results.</li>
                            <li>GPU acceleration can deliver 5&ndash;50&times; speedups for both training and search workloads.</li>
                        </ul>

                        <p>Vector search is the backbone of retrieval-augmented AI. Understanding the trade-offs between exact and approximate methods, and knowing how to combine lexical and semantic signals, is essential for building robust, production-grade AI systems.</p>

                    </div>
                </div>

                <aside class="article-sidebar">
                    <div class="sidebar-author">
                        <img src="images/pm.png" alt="Paolo Mascia" class="sidebar-author-photo">
                        <div class="sidebar-author-name">Paolo Mascia</div>
                        <div class="sidebar-author-role">AI &amp; Cloud Architect</div>
                        <p class="sidebar-author-bio">25+ years designing large-scale distributed systems. Specialized in Generative AI, AI Agents, and cloud-native architectures.</p>
                        <div class="sidebar-author-links">
                            <a href="https://www.linkedin.com/in/paolo-mascia-italy" target="_blank" aria-label="LinkedIn">
                                <svg viewBox="0 0 24 24"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 01-2.063-2.065 2.064 2.064 0 112.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/></svg>
                            </a>
                            <a href="https://github.com/paolomascia" target="_blank" aria-label="GitHub">
                                <svg viewBox="0 0 24 24"><path d="M12 0C5.374 0 0 5.373 0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23A11.509 11.509 0 0112 5.803c1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg>
                            </a>
                            <a href="https://www.kaggle.com/paolomascia" target="_blank" aria-label="Kaggle">
                                <svg viewBox="0 0 24 24"><path d="M18.825 23.859c-.022.092-.117.141-.281.141h-3.139c-.187 0-.351-.082-.492-.248l-5.178-6.589-1.448 1.374v5.111c0 .235-.117.352-.351.352H5.505c-.236 0-.354-.117-.354-.352V.353c0-.233.118-.353.354-.353h2.431c.234 0 .351.12.351.353v14.343l6.203-6.272c.165-.165.33-.246.495-.246h3.239c.144 0 .236.06.281.18.046.149.034.233-.036.315l-6.555 6.344 6.836 8.507c.095.104.117.208.075.378z"/></svg>
                            </a>
                        </div>
                    </div>
                    <div class="sidebar-box">
                        <h4>More Articles</h4>
                        <a href="post-rag.html" class="sidebar-link">
                            <span class="sidebar-link-tag">RAG</span>
                            <div class="sidebar-link-title">Retrieval-Augmented Generation (RAG)</div>
                            <span class="sidebar-link-meta">Feb 2025</span>
                        </a>
                        <a href="post-transformers.html" class="sidebar-link">
                            <span class="sidebar-link-tag">Transformers</span>
                            <div class="sidebar-link-title">Transformers: The Architecture That Revolutionized Deep Learning</div>
                            <span class="sidebar-link-meta">Mar 2025</span>
                        </a>
                        <a href="post-pca.html" class="sidebar-link">
                            <span class="sidebar-link-tag">PCA</span>
                            <div class="sidebar-link-title">Principal Component Analysis (PCA) in Python</div>
                            <span class="sidebar-link-meta">May 2024</span>
                        </a>
                        <a href="post-tsne-umap.html" class="sidebar-link">
                            <span class="sidebar-link-tag">Dimension Reduction</span>
                            <div class="sidebar-link-title">t-SNE and UMAP in Python</div>
                            <span class="sidebar-link-meta">Jul 2024</span>
                        </a>
                        <a href="post-agentic-ai.html" class="sidebar-link">
                            <span class="sidebar-link-tag">Agentic AI</span>
                            <div class="sidebar-link-title">The Rise of Agentic AI: Why 2025 Changed Everything</div>
                            <span class="sidebar-link-meta">Sep 2025</span>
                        </a>
                    </div>
                </aside>
            </div>
        </div>
    </article>

    <footer class="footer">
        <div class="container">
            <p>&copy; 2025 Paolo Mascia. Built with curiosity and too much coffee.</p>
        </div>
    </footer>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
</body>
</html>